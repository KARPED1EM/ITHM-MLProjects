{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# AttriPredict - Brute-Force Model Search\n",
        "\n",
        "This notebook performs an exhaustive brute-force search over models, hyperparameters, feature engineering, and random seeds.\n",
        "\n",
        "> Baseline: example.ipynb: 0.8797 (best model found during EDA)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# ============================================================================\n",
        "# Environment Setup\n",
        "# ============================================================================\n",
        "import warnings\n",
        "from sklearn.exceptions import ConvergenceWarning\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
        "\n",
        "import ast\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "import joblib\n",
        "\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold, RepeatedStratifiedKFold\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler, OrdinalEncoder, PolynomialFeatures\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier, HistGradientBoostingClassifier\n",
        "from sklearn.feature_selection import RFECV, SelectKBest, f_classif\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.exceptions import ConvergenceWarning\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "\n",
        "try:\n",
        "    from imblearn.over_sampling import SMOTE, ADASYN, BorderlineSMOTE\n",
        "    from imblearn.combine import SMOTETomek, SMOTEENN\n",
        "    from imblearn.ensemble import BalancedRandomForestClassifier\n",
        "    HAS_IMB = True\n",
        "except Exception:\n",
        "    HAS_IMB = False\n",
        "    SMOTE = ADASYN = BorderlineSMOTE = SMOTETomek = SMOTEENN = None\n",
        "    BalancedRandomForestClassifier = EasyEnsembleClassifier = None\n",
        "    print(\"[WARN] imblearn not available, skipping advanced samplers\")\n",
        "\n",
        "\n",
        "try:\n",
        "    import lightgbm as lgb\n",
        "    HAS_LGB = True\n",
        "except Exception:\n",
        "    HAS_LGB = False\n",
        "    lgb = None\n",
        "    print(\"[WARN] lightgbm not available\")\n",
        "\n",
        "\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "    HAS_XGB = True\n",
        "except Exception:\n",
        "    HAS_XGB = False\n",
        "    xgb = None\n",
        "    print(\"[WARN] xgboost not available\")\n",
        "\n",
        "\n",
        "try:\n",
        "    import catboost as cb\n",
        "    HAS_CAT = True\n",
        "except Exception:\n",
        "    HAS_CAT = False\n",
        "    cb = None\n",
        "    print(\"[WARN] catboost not available\")\n",
        "\n",
        "\n",
        "# Global randomness control\n",
        "RANDOM_STATE = 42\n",
        "RANDOM_SEEDS = [42, 2025]\n",
        "np.random.seed(RANDOM_STATE)\n",
        "\n",
        "\n",
        "# Artifact roots (mirrors 02 project layout)\n",
        "ARTIFACT_ROOT = Path('artifacts/fair_bruteforce')\n",
        "SUMMARY_DIR = ARTIFACT_ROOT / 'summaries'\n",
        "GLOBAL_FIG_DIR = ARTIFACT_ROOT / 'global_figures'\n",
        "for path in [ARTIFACT_ROOT, SUMMARY_DIR, GLOBAL_FIG_DIR]:\n",
        "    path.mkdir(parents=True, exist_ok=True)\n",
        "FAMILY_ARTIFACTS = {}\n",
        "\n",
        "\n",
        "# In notebook experiment tracker\n",
        "EXPERIMENT_LOG = []\n",
        "\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"AttriPredict Production Ultimate Initialized\")\n",
        "print(f\"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "print(f\"Libraries: IMB={HAS_IMB}, LGB={HAS_LGB}, XGB={HAS_XGB}, CAT={HAS_CAT}\")\n",
        "print(f\"Artifacts root: {ARTIFACT_ROOT.resolve()}\")\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# ============================================================================\n",
        "# Load Data\n",
        "# ============================================================================\n",
        "print(\"\\n[1/8] Loading Data...\")\n",
        "\n",
        "train_data = pd.read_csv('../../data/train.csv')\n",
        "test_data = pd.read_csv('../../data/test.csv')\n",
        "\n",
        "print(f\"✓ Train shape: {train_data.shape}\")\n",
        "print(f\"✓ Test shape: {test_data.shape}\")\n",
        "\n",
        "drop_cols = ['Over18', 'StandardHours', 'EmployeeNumber']\n",
        "train_data = train_data.drop(drop_cols, axis=1)\n",
        "test_data = test_data.drop(drop_cols, axis=1)\n",
        "\n",
        "X_train_raw = train_data.drop('Attrition', axis=1)\n",
        "y_train_raw = train_data['Attrition']\n",
        "X_test_raw = test_data.drop('Attrition', axis=1)\n",
        "y_test_raw = test_data['Attrition']\n",
        "\n",
        "NOMINAL_CATEGORICALS = [\n",
        "    'BusinessTravel',\n",
        "    'Department',\n",
        "    'EducationField',\n",
        "    'Gender',\n",
        "    'JobRole',\n",
        "    'MaritalStatus',\n",
        "    'OverTime'\n",
        "]\n",
        "NUMERIC_FEATURES = [col for col in X_train_raw.columns if col not in NOMINAL_CATEGORICALS]\n",
        "\n",
        "print(f\"✓ Nominal categorical features: {len(NOMINAL_CATEGORICALS)}\")\n",
        "print(f\"✓ Numeric features: {len(NUMERIC_FEATURES)}\")\n",
        "\n",
        "SUMMARY_DIR.mkdir(parents=True, exist_ok=True)\n",
        "pd.Series({\n",
        "    'train_rows': len(train_data),\n",
        "    'test_rows': len(test_data),\n",
        "    'feature_count': X_train_raw.shape[1]\n",
        "}).to_frame('value').to_csv(SUMMARY_DIR / 'dataset_overview.csv')\n",
        "y_train_raw.value_counts().to_csv(SUMMARY_DIR / 'train_class_balance.csv', header=['count'])\n",
        "train_data.describe(include='all').transpose().to_csv(SUMMARY_DIR / 'train_describe.csv')\n",
        "test_data.describe(include='all').transpose().to_csv(SUMMARY_DIR / 'test_describe.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# ============================================================================\n",
        "# Feature Engineering - Level 1: Basic Preprocessing\n",
        "# ============================================================================\n",
        "print(\"\\n[2/8] Feature Engineering - Level 1: Basic Preprocessing...\")\n",
        "\n",
        "\n",
        "def get_sampler(sampler_config):\n",
        "    if sampler_config is None or not HAS_IMB:\n",
        "        return None\n",
        "    sampler_name = sampler_config.get('name', 'smote').lower()\n",
        "    sampler_params = dict(sampler_config.get('params', {}))\n",
        "    sampler_params.setdefault('random_state', sampler_config.get('random_state', RANDOM_STATE))\n",
        "\n",
        "    if sampler_name == 'smote' and SMOTE is not None:\n",
        "        return SMOTE(**sampler_params)\n",
        "    if sampler_name == 'adasyn' and ADASYN is not None:\n",
        "        return ADASYN(**sampler_params)\n",
        "    if sampler_name == 'borderlinesmote' and BorderlineSMOTE is not None:\n",
        "        return BorderlineSMOTE(**sampler_params)\n",
        "    if sampler_name == 'smoteenn' and SMOTEENN is not None:\n",
        "        return SMOTEENN(**sampler_params)\n",
        "    if sampler_name == 'smotetomek' and SMOTETomek is not None:\n",
        "        return SMOTETomek(**sampler_params)\n",
        "\n",
        "    raise ValueError(f\"Unsupported sampler '{sampler_name}' or imblearn component missing.\")\n",
        "\n",
        "\n",
        "def basic_preprocess(\n",
        "    X_train,\n",
        "    extra_datasets=None,\n",
        "    sampler_config=None,\n",
        "    y_train=None,\n",
        "    verbose=True\n",
        "):\n",
        "    \"\"\"\n",
        "    Basic preprocessing pipeline: encoding + log1p on skewed numerics + scaling,\n",
        "    with optional sampler applied at the basic stage.\n",
        "    \"\"\"\n",
        "    if extra_datasets is None:\n",
        "        extra_datasets = []\n",
        "\n",
        "    nominal_features = NOMINAL_CATEGORICALS\n",
        "\n",
        "    if verbose:\n",
        "        print(\"  [Basic] OneHot encoding categorical features...\")\n",
        "\n",
        "    encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
        "\n",
        "    encoded_train = encoder.fit_transform(X_train[nominal_features])\n",
        "    encoded_df_train = pd.DataFrame(\n",
        "        encoded_train,\n",
        "        columns=encoder.get_feature_names_out(nominal_features),\n",
        "        index=X_train.index\n",
        "    )\n",
        "    X_train_proc = X_train.drop(nominal_features, axis=1).copy()\n",
        "    X_train_proc = pd.concat([X_train_proc, encoded_df_train], axis=1)\n",
        "\n",
        "    processed_extras = []\n",
        "    for X_extra in extra_datasets:\n",
        "        encoded_extra = encoder.transform(X_extra[nominal_features])\n",
        "        encoded_df_extra = pd.DataFrame(\n",
        "            encoded_extra, # type: ignore\n",
        "            columns=encoder.get_feature_names_out(nominal_features),\n",
        "            index=X_extra.index\n",
        "        )\n",
        "        X_extra_proc = X_extra.drop(nominal_features, axis=1).copy()\n",
        "        X_extra_proc = pd.concat([X_extra_proc, encoded_df_extra], axis=1)\n",
        "        processed_extras.append(X_extra_proc)\n",
        "\n",
        "    feature_columns = X_train_proc.columns\n",
        "    encoded_count = encoded_df_train.shape[1]\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"    [Basic] Encoded features added: {encoded_count}\")\n",
        "        print(\"  [Basic] Handling skewness with log1p...\")\n",
        "\n",
        "    numeric_cols = X_train_proc.select_dtypes(include=['float64', 'int64']).columns\n",
        "    skewed_features = []\n",
        "\n",
        "    for col in numeric_cols:\n",
        "        skewness = X_train_proc[col].skew()\n",
        "        if abs(skewness) > 0.5: # type: ignore\n",
        "            X_train_proc[col] = np.log1p(X_train_proc[col])\n",
        "            for dataset in processed_extras:\n",
        "                if col in dataset.columns:\n",
        "                    dataset[col] = np.log1p(dataset[col])\n",
        "            skewed_features.append(col)\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"    [Basic] Log-transformed: {len(skewed_features)} features\")\n",
        "        print(\"  [Basic] Standardizing features...\")\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = pd.DataFrame(\n",
        "        scaler.fit_transform(X_train_proc),\n",
        "        columns=feature_columns,\n",
        "        index=X_train_proc.index\n",
        "    )\n",
        "\n",
        "    scaled_extras = []\n",
        "    for dataset in processed_extras:\n",
        "        scaled_dataset = pd.DataFrame(\n",
        "            scaler.transform(dataset[feature_columns]),\n",
        "            columns=feature_columns,\n",
        "            index=dataset.index\n",
        "        )\n",
        "        scaled_extras.append(scaled_dataset)\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"    [Basic] Scaled feature space: {X_train_scaled.shape[1]} columns\")\n",
        "\n",
        "    sampler = get_sampler(sampler_config)\n",
        "    y_train_final = y_train\n",
        "\n",
        "    if sampler is not None and y_train is not None:\n",
        "        if verbose:\n",
        "            print(f\"  [Basic] Applying sampler: {sampler.__class__.__name__} ...\")\n",
        "        X_resampled, y_resampled = sampler.fit_resample(X_train_scaled.values, np.asarray(y_train)) # type: ignore\n",
        "        X_train_scaled = pd.DataFrame(X_resampled, columns=feature_columns)\n",
        "        y_train_final = y_resampled\n",
        "        if verbose:\n",
        "            print(f\"    [Basic] Samples: {len(np.asarray(y_train))} -> {len(y_resampled)}\")\n",
        "\n",
        "    return X_train_scaled, scaled_extras, y_train_final\n",
        "\n",
        "\n",
        "print(\"  ✓ Basic preprocessing function enhanced\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# ============================================================================\n",
        "# Feature Engineering - Level 2: Advanced Features\n",
        "# ============================================================================\n",
        "print(\"\\n[3/8] Feature Engineering - Level 2: Advanced Features...\")\n",
        "\n",
        "\n",
        "def create_advanced_features(X_train, extra_datasets=None, advanced_config=None, verbose=True):\n",
        "    \"\"\"Create interaction, ratio, delta, and aggregation features.\"\"\"\n",
        "    if extra_datasets is None:\n",
        "        extra_datasets = []\n",
        "    if advanced_config is None:\n",
        "        advanced_config = {}\n",
        "\n",
        "    X_train_adv = X_train.copy()\n",
        "    extras_adv = [dataset.copy() for dataset in extra_datasets]\n",
        "\n",
        "    interaction_pairs = advanced_config.get('interaction_pairs', [\n",
        "        ('Age', 'MonthlyIncome'),\n",
        "        ('DistanceFromHome', 'OverTime_Yes'),\n",
        "        ('YearsAtCompany', 'JobLevel'),\n",
        "        ('TotalWorkingYears', 'Age'),\n",
        "        ('WorkLifeBalance', 'OverTime_Yes'),\n",
        "        ('JobSatisfaction', 'EnvironmentSatisfaction'),\n",
        "        ('TotalWorkingYears', 'NumCompaniesWorked'),\n",
        "        ('MonthlyIncome', 'JobLevel'),\n",
        "    ])\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"  [Advanced] Creating {len(interaction_pairs)} interaction features...\")\n",
        "\n",
        "    for col1, col2 in interaction_pairs:\n",
        "        if col1 in X_train_adv.columns and col2 in X_train_adv.columns:\n",
        "            feat_name = f\"{col1}_x_{col2}\"\n",
        "            X_train_adv[feat_name] = X_train_adv[col1] * X_train_adv[col2]\n",
        "            for dataset in extras_adv:\n",
        "                if col1 in dataset.columns and col2 in dataset.columns:\n",
        "                    dataset[feat_name] = dataset[col1] * dataset[col2]\n",
        "\n",
        "    ratio_specs = advanced_config.get('ratio_features', [\n",
        "        ('MonthlyIncome', 'Age', 'Income_per_Age'),\n",
        "        ('YearsAtCompany', 'TotalWorkingYears', 'Company_vs_Total_Years'),\n",
        "        ('YearsSinceLastPromotion', 'YearsAtCompany', 'Promo_Frequency'),\n",
        "        ('YearsWithCurrManager', 'YearsInCurrentRole', 'Manager_Stability'),\n",
        "        ('TotalWorkingYears', 'NumCompaniesWorked', 'Years_per_Company'),\n",
        "    ])\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"  [Advanced] Creating {len(ratio_specs)} ratio features...\")\n",
        "\n",
        "    for num_col, denom_col, feat_name in ratio_specs:\n",
        "        if num_col in X_train_adv.columns and denom_col in X_train_adv.columns:\n",
        "            X_train_adv[feat_name] = X_train_adv[num_col] / (X_train_adv[denom_col] + 1e-5)\n",
        "            for dataset in extras_adv:\n",
        "                if num_col in dataset.columns and denom_col in dataset.columns:\n",
        "                    dataset[feat_name] = dataset[num_col] / (dataset[denom_col] + 1e-5)\n",
        "\n",
        "    delta_specs = advanced_config.get('delta_features', [\n",
        "        ('YearsAtCompany', 'YearsInCurrentRole', 'Tenure_vs_Role'),\n",
        "        ('YearsAtCompany', 'YearsWithCurrManager', 'Tenure_vs_Manager'),\n",
        "        ('PerformanceRating', 'EnvironmentSatisfaction', 'Performance_vs_Environment'),\n",
        "    ])\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"  [Advanced] Creating {len(delta_specs)} delta features...\")\n",
        "\n",
        "    for top_col, bottom_col, feat_name in delta_specs:\n",
        "        if top_col in X_train_adv.columns and bottom_col in X_train_adv.columns:\n",
        "            X_train_adv[feat_name] = X_train_adv[top_col] - X_train_adv[bottom_col]\n",
        "            for dataset in extras_adv:\n",
        "                if top_col in dataset.columns and bottom_col in dataset.columns:\n",
        "                    dataset[feat_name] = dataset[top_col] - dataset[bottom_col]\n",
        "\n",
        "    satisfaction_cols = ['JobSatisfaction', 'EnvironmentSatisfaction',\n",
        "                         'RelationshipSatisfaction', 'WorkLifeBalance']\n",
        "\n",
        "    if all(col in X_train_adv.columns for col in satisfaction_cols):\n",
        "        if verbose:\n",
        "            print(\"  [Advanced] Aggregating satisfaction metrics...\")\n",
        "        X_train_adv['Satisfaction_Mean'] = X_train_adv[satisfaction_cols].mean(axis=1)\n",
        "        X_train_adv['Satisfaction_Std'] = X_train_adv[satisfaction_cols].std(axis=1)\n",
        "        X_train_adv['Satisfaction_Max'] = X_train_adv[satisfaction_cols].max(axis=1)\n",
        "        for dataset in extras_adv:\n",
        "            dataset['Satisfaction_Mean'] = dataset[satisfaction_cols].mean(axis=1)\n",
        "            dataset['Satisfaction_Std'] = dataset[satisfaction_cols].std(axis=1)\n",
        "            dataset['Satisfaction_Max'] = dataset[satisfaction_cols].max(axis=1)\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"  [Advanced] Total features after expansion: {X_train_adv.shape[1]}\")\n",
        "\n",
        "    return X_train_adv, extras_adv\n",
        "\n",
        "\n",
        "def apply_polynomial_features(X_train, extra_datasets=None, poly_config=None, verbose=True):\n",
        "    \"\"\"Apply polynomial feature expansion on a subset of columns.\"\"\"\n",
        "    if poly_config is None:\n",
        "        return X_train, extra_datasets or [], []\n",
        "\n",
        "    if extra_datasets is None:\n",
        "        extra_datasets = []\n",
        "\n",
        "    max_base = poly_config.get('max_base_features', 12)\n",
        "    degree = poly_config.get('degree', 2)\n",
        "    interaction_only = poly_config.get('interaction_only', False)\n",
        "    include_bias = poly_config.get('include_bias', False)\n",
        "\n",
        "    numeric_cols = list(X_train.select_dtypes(include=[np.number]).columns)\n",
        "    base_cols = poly_config.get('columns')\n",
        "    if base_cols is None:\n",
        "        std_series = X_train[numeric_cols].std().sort_values(ascending=False)\n",
        "        base_cols = list(std_series.head(max_base).index)\n",
        "    else:\n",
        "        base_cols = [col for col in base_cols if col in X_train.columns]\n",
        "\n",
        "    if len(base_cols) == 0:\n",
        "        return X_train, extra_datasets, []\n",
        "\n",
        "    poly = PolynomialFeatures(\n",
        "        degree=degree,\n",
        "        interaction_only=interaction_only,\n",
        "        include_bias=include_bias\n",
        "    )\n",
        "\n",
        "    train_poly = poly.fit_transform(X_train[base_cols])\n",
        "    poly_feature_names = poly.get_feature_names_out(base_cols)\n",
        "    poly_df_train = pd.DataFrame(train_poly, columns=poly_feature_names, index=X_train.index)\n",
        "\n",
        "    new_columns = [col for col in poly_df_train.columns if col not in X_train.columns]\n",
        "    poly_df_train = poly_df_train[new_columns]\n",
        "    X_train_poly = pd.concat([X_train, poly_df_train], axis=1)\n",
        "\n",
        "    extras_poly = []\n",
        "    for dataset in extra_datasets:\n",
        "        transformed = poly.transform(dataset[base_cols]) # type: ignore\n",
        "        poly_df_extra = pd.DataFrame(transformed, columns=poly_feature_names, index=dataset.index) # type: ignore\n",
        "        poly_df_extra = poly_df_extra[new_columns]\n",
        "        extras_poly.append(pd.concat([dataset, poly_df_extra], axis=1))\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"  [Poly] Degree {degree} expansion on {len(base_cols)} base cols -> {len(new_columns)} new cols\")\n",
        "\n",
        "    return X_train_poly, extras_poly, new_columns\n",
        "\n",
        "\n",
        "def apply_feature_selector(X_train, y_train, extra_datasets=None, selector_config=None, verbose=True):\n",
        "    \"\"\"Apply a feature selection strategy and keep aligned columns for extras.\"\"\"\n",
        "    if selector_config is None:\n",
        "        return X_train, extra_datasets or [], X_train.columns.tolist(), \"none\"\n",
        "\n",
        "    if extra_datasets is None:\n",
        "        extra_datasets = []\n",
        "\n",
        "    strategy = selector_config.get('strategy', 'selectk').lower()\n",
        "    summary = 'none'\n",
        "    support_mask = None\n",
        "\n",
        "    if strategy == 'selectk':\n",
        "        k = selector_config.get('k', min(120, X_train.shape[1]))\n",
        "        k = min(k, X_train.shape[1])\n",
        "        selector = SelectKBest(score_func=f_classif, k=k)\n",
        "        selector.fit(X_train, y_train)\n",
        "        support_mask = selector.get_support()\n",
        "        summary = f'selectk_{k}'\n",
        "    elif strategy == 'rfecv':\n",
        "        step = selector_config.get('step', 1)\n",
        "        min_features = selector_config.get('min_features', max(25, X_train.shape[1] // 6))\n",
        "        base_estimator = selector_config.get('estimator')\n",
        "        if base_estimator is None:\n",
        "            base_estimator = LogisticRegression(C=1.0, penalty='l2', solver='lbfgs', max_iter=2000)\n",
        "        selector = RFECV(\n",
        "            estimator=base_estimator,\n",
        "            step=step,\n",
        "            cv=StratifiedKFold(n_splits=3, shuffle=True, random_state=RANDOM_STATE),\n",
        "            scoring='roc_auc',\n",
        "            min_features_to_select=min_features,\n",
        "            n_jobs=-1\n",
        "        )\n",
        "        selector.fit(X_train, y_train)\n",
        "        support_mask = selector.get_support()\n",
        "        summary = f'rfecv_step{step}_min{min_features}'\n",
        "    elif strategy == 'l1':\n",
        "        C_value = selector_config.get('C', 0.5)\n",
        "        max_iter = selector_config.get('max_iter', 4000)\n",
        "        threshold = selector_config.get('threshold', 1e-6)\n",
        "        min_features = selector_config.get('min_features', min(40, X_train.shape[1]))\n",
        "        base_estimator = LogisticRegression(\n",
        "            penalty='l1',\n",
        "            solver='saga',\n",
        "            C=C_value,\n",
        "            max_iter=max_iter,\n",
        "            random_state=selector_config.get('random_state', RANDOM_STATE)\n",
        "        )\n",
        "        base_estimator.fit(X_train, np.asarray(y_train))\n",
        "        coefs = np.abs(base_estimator.coef_).ravel()\n",
        "        mask = coefs > threshold\n",
        "        if not mask.any():\n",
        "            top_idx = np.argsort(coefs)[-min_features:]\n",
        "            mask = np.zeros_like(coefs, dtype=bool)\n",
        "            mask[top_idx] = True\n",
        "        support_mask = mask\n",
        "        summary = f'l1_{support_mask.sum()}'\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown feature selector strategy: {strategy}\")\n",
        "\n",
        "    selected_columns = X_train.columns[support_mask]\n",
        "\n",
        "    X_train_selected = X_train[selected_columns].copy()\n",
        "    extras_selected = [dataset[selected_columns].copy() for dataset in extra_datasets]\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"  [Selector] Strategy={summary} retained {len(selected_columns)} columns\")\n",
        "\n",
        "    return X_train_selected, extras_selected, selected_columns.tolist(), summary\n",
        "\n",
        "\n",
        "print(\"  ✓ Advanced feature engineering utilities ready\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# Experiment Tracking System\n",
        "# ============================================================================\n",
        "print(\"\\n[4/8] Setting up Experiment Tracking...\")\n",
        "\n",
        "\n",
        "def log_experiment(exp_name, model_name, cv_auc_mean, cv_auc_std, test_auc,\n",
        "                   features_used, hyperparams, notes=\"\", metadata=None):\n",
        "    \"\"\"Persist experiment metadata into the global experiment log.\"\"\"\n",
        "    experiment = {\n",
        "        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "        'exp_name': exp_name,\n",
        "        'model': model_name,\n",
        "        'cv_auc_mean': cv_auc_mean,\n",
        "        'cv_auc_std': cv_auc_std,\n",
        "        'test_auc': test_auc,\n",
        "        'features_count': features_used,\n",
        "        'hyperparams': str(hyperparams),\n",
        "        'notes': notes\n",
        "    }\n",
        "\n",
        "    if metadata:\n",
        "        experiment.update(metadata)\n",
        "\n",
        "    EXPERIMENT_LOG.append(experiment)\n",
        "    print(f\"  -> Logged: {exp_name} | CV: {cv_auc_mean:.4f}±{cv_auc_std:.4f} | Test: {test_auc:.4f}\")\n",
        "    return experiment\n",
        "\n",
        "\n",
        "def get_results_df():\n",
        "    \"\"\"Return experiments as sorted DataFrame.\"\"\"\n",
        "    df = pd.DataFrame(EXPERIMENT_LOG)\n",
        "    if len(df) > 0:\n",
        "        df = df.sort_values('test_auc', ascending=False)\n",
        "    return df\n",
        "\n",
        "\n",
        "print(\"  ✓ Experiment tracking system ready\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# ============================================================================\n",
        "# Cross-Validation Utilities\n",
        "# ============================================================================\n",
        "print(\"  ✓ CV evaluation function enhanced\")\n",
        "\n",
        "\n",
        "def evaluate_with_cv(model, X, y, X_test, y_test, feature_config, pipeline_key, cv_override=None):\n",
        "    \"\"\"\n",
        "    Perform cross-validation with dynamic feature preparation and aggregate\n",
        "    test performance using the configured CV strategy.\n",
        "    \"\"\"\n",
        "    cv_mode = feature_config.get('cv_mode', 'kfold')\n",
        "    n_folds = feature_config.get('n_folds', 5)\n",
        "\n",
        "    if cv_override is not None:\n",
        "        splitter = cv_override\n",
        "    elif cv_mode == 'repeated_5x2':\n",
        "        splitter = RepeatedStratifiedKFold(n_splits=5, n_repeats=2, random_state=RANDOM_STATE)\n",
        "    elif cv_mode == 'kfold10':\n",
        "        splitter = StratifiedKFold(n_splits=10, shuffle=True, random_state=RANDOM_STATE)\n",
        "    else:\n",
        "        splitter = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=RANDOM_STATE)\n",
        "\n",
        "    cv_scores = []\n",
        "    test_preds = np.zeros(len(X_test))\n",
        "    fold_count = 0\n",
        "    feature_count = None\n",
        "    last_metadata = {}\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(splitter.split(X, y)):\n",
        "        fold_count += 1\n",
        "        X_fold_train = X.iloc[train_idx]\n",
        "        X_fold_val = X.iloc[val_idx]\n",
        "\n",
        "        if hasattr(y, 'iloc'):\n",
        "            y_fold_train = y.iloc[train_idx]\n",
        "            y_fold_val = y.iloc[val_idx]\n",
        "        else:\n",
        "            y_fold_train = y[train_idx]\n",
        "            y_fold_val = y[val_idx]\n",
        "\n",
        "        prep_tuple = prepare_feature_set( # type: ignore\n",
        "            pipeline_key,\n",
        "            feature_config,\n",
        "            X_fold_train,\n",
        "            X_fold_val,\n",
        "            X_test,\n",
        "            y_fold_train\n",
        "        )\n",
        "\n",
        "        if len(prep_tuple) == 4:\n",
        "            X_train_ready, X_val_ready, X_test_ready, y_train_ready = prep_tuple\n",
        "            prep_meta = {}\n",
        "        else:\n",
        "            X_train_ready, X_val_ready, X_test_ready, y_train_ready, prep_meta = prep_tuple\n",
        "\n",
        "        feature_count = X_train_ready.shape[1]\n",
        "        last_metadata = prep_meta\n",
        "        y_val_array = np.asarray(y_fold_val)\n",
        "\n",
        "        model_clone = model.__class__(**model.get_params()) if hasattr(model, 'get_params') else model\n",
        "        model_clone.fit(X_train_ready, y_train_ready)\n",
        "\n",
        "        val_pred = model_clone.predict_proba(X_val_ready)[:, 1]\n",
        "        val_auc = roc_auc_score(y_val_array, val_pred)\n",
        "        cv_scores.append(val_auc)\n",
        "\n",
        "        fold_test_pred = model_clone.predict_proba(X_test_ready)[:, 1]\n",
        "        test_preds += fold_test_pred\n",
        "\n",
        "    cv_mean = float(np.mean(cv_scores)) if cv_scores else 0.0\n",
        "    cv_std = float(np.std(cv_scores)) if cv_scores else 0.0\n",
        "    test_preds /= max(fold_count, 1)\n",
        "    test_auc = roc_auc_score(np.asarray(y_test), test_preds)\n",
        "\n",
        "    return cv_mean, cv_std, test_auc, test_preds, feature_count, last_metadata\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# ============================================================================\n",
        "# Feature Pipelines & Routing\n",
        "# ============================================================================\n",
        "print(\"\\n[5/8] Building family-specific feature pipelines...\")\n",
        "\n",
        "MODEL_FAMILIES = {\n",
        "    'linear': ['LR', 'SVC'],\n",
        "    'tree': ['XGB', 'LGB', 'CAT', 'RF', 'GB', 'HGB', 'ET', 'BRF'],\n",
        "    'neural': ['MLP']\n",
        "}\n",
        "MODEL_TO_FAMILY = {model: family for family, models in MODEL_FAMILIES.items() for model in models}\n",
        "\n",
        "LINEAR_PIPELINES = {\n",
        "    'linear_basic': {\n",
        "        'family': 'linear',\n",
        "        'description': 'Scaled OHE + log1p base features (no manual extras)',\n",
        "        'cv_mode': 'kfold',\n",
        "        'n_folds': 5,\n",
        "        'options': {\n",
        "            'use_advanced': False,\n",
        "            'samplers': {'basic': {'name': 'smote'}}\n",
        "        }\n",
        "    },\n",
        "    'linear_advanced': {\n",
        "        'family': 'linear',\n",
        "        'description': 'Scaled base + engineered interactions',\n",
        "        'cv_mode': 'kfold',\n",
        "        'n_folds': 5,\n",
        "        'options': {\n",
        "            'use_advanced': True,\n",
        "            'samplers': {'basic': {'name': 'smote'}}\n",
        "        }\n",
        "    },\n",
        "    'linear_poly': {\n",
        "        'family': 'linear',\n",
        "        'description': 'Advanced features + polynomial bump',\n",
        "        'cv_mode': 'kfold',\n",
        "        'n_folds': 5,\n",
        "        'options': {\n",
        "            'use_advanced': True,\n",
        "            'poly': {'degree': 2, 'interaction_only': False, 'include_bias': False, 'max_base_features': 16},\n",
        "            'samplers': {'basic': {'name': 'smote'}}\n",
        "        }\n",
        "    },\n",
        "    'linear_sparse': {\n",
        "        'family': 'linear',\n",
        "        'description': 'Advanced features + L1 feature squeeze',\n",
        "        'cv_mode': 'kfold',\n",
        "        'n_folds': 5,\n",
        "        'options': {\n",
        "            'use_advanced': True,\n",
        "            'feature_selector': {'strategy': 'l1', 'C': 0.8, 'threshold': 1e-5, 'min_features': 60},\n",
        "            'samplers': {'basic': {'name': 'smote'}}\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "TREE_PIPELINES = {\n",
        "    'tree_raw': {\n",
        "        'family': 'tree',\n",
        "        'description': 'Label encoding + raw numeric signals',\n",
        "        'cv_mode': 'kfold',\n",
        "        'n_folds': 5,\n",
        "        'options': {'encoding': 'label', 'use_smote': False}\n",
        "    },\n",
        "    'tree_raw_smote': {\n",
        "        'family': 'tree',\n",
        "        'description': 'Label encoding + SMOTE uplift',\n",
        "        'cv_mode': 'kfold',\n",
        "        'n_folds': 5,\n",
        "        'options': {'encoding': 'label', 'use_smote': True, 'sampler': {'name': 'smote', 'params': {'k_neighbors': 4}}}\n",
        "    },\n",
        "    'tree_target': {\n",
        "        'family': 'tree',\n",
        "        'description': 'Target encoding on categoricals',\n",
        "        'cv_mode': 'kfold',\n",
        "        'n_folds': 5,\n",
        "        'options': {'encoding': 'target', 'use_smote': False}\n",
        "    },\n",
        "    'tree_onehot': {\n",
        "        'family': 'tree',\n",
        "        'description': 'One-hot encoding without scaling',\n",
        "        'cv_mode': 'kfold',\n",
        "        'n_folds': 5,\n",
        "        'options': {'encoding': 'onehot', 'use_smote': False}\n",
        "    }\n",
        "}\n",
        "\n",
        "NEURAL_PIPELINES = {\n",
        "    'neural_standard': {\n",
        "        'family': 'neural',\n",
        "        'description': 'One-hot + StandardScaler + SMOTE',\n",
        "        'cv_mode': 'kfold',\n",
        "        'n_folds': 5,\n",
        "        'options': {'scaler': 'standard', 'use_smote': True, 'sampler': {'name': 'smote', 'params': {'k_neighbors': 5}}}\n",
        "    },\n",
        "    'neural_minmax': {\n",
        "        'family': 'neural',\n",
        "        'description': 'One-hot + MinMaxScaler + SMOTE',\n",
        "        'cv_mode': 'kfold',\n",
        "        'n_folds': 5,\n",
        "        'options': {'scaler': 'minmax', 'use_smote': True, 'sampler': {'name': 'smote', 'params': {'k_neighbors': 6}}}\n",
        "    },\n",
        "    'neural_raw': {\n",
        "        'family': 'neural',\n",
        "        'description': 'One-hot + StandardScaler (no sampling)',\n",
        "        'cv_mode': 'kfold',\n",
        "        'n_folds': 5,\n",
        "        'options': {'scaler': 'standard', 'use_smote': False}\n",
        "    }\n",
        "}\n",
        "\n",
        "FEATURE_PIPELINES = {**LINEAR_PIPELINES, **TREE_PIPELINES, **NEURAL_PIPELINES}\n",
        "\n",
        "print(f\"  ✓ Linear pipelines: {', '.join(LINEAR_PIPELINES.keys())}\")\n",
        "print(f\"  ✓ Tree pipelines: {', '.join(TREE_PIPELINES.keys())}\")\n",
        "print(f\"  ✓ Neural pipelines: {', '.join(NEURAL_PIPELINES.keys())}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def init_family_artifacts():\n",
        "    global FAMILY_ARTIFACTS\n",
        "    FAMILY_ARTIFACTS = {}\n",
        "    for family in MODEL_FAMILIES:\n",
        "        base = ARTIFACT_ROOT / family\n",
        "        data_dir = base / 'data'\n",
        "        fig_dir = base / 'figures'\n",
        "        model_dir = base / 'models'\n",
        "        for artifact_path in [base, data_dir, fig_dir, model_dir]:\n",
        "            artifact_path.mkdir(parents=True, exist_ok=True)\n",
        "        FAMILY_ARTIFACTS[family] = {\n",
        "            'base': base,\n",
        "            'data': data_dir,\n",
        "            'figures': fig_dir,\n",
        "            'models': model_dir\n",
        "        }\n",
        "\n",
        "\n",
        "init_family_artifacts()\n",
        "def prepare_linear_features(pipeline_name, config, X_train, X_val, X_test, y_train):\n",
        "    options = dict(config.get('options', {}))\n",
        "    samplers = options.get('samplers', {})\n",
        "    basic_sampler_cfg = samplers.get('basic')\n",
        "    advanced_sampler_cfg = samplers.get('advanced')\n",
        "    extra_datasets = [X_val, X_test]\n",
        "\n",
        "    X_train_ready, extras_basic, y_train_ready = basic_preprocess(\n",
        "        X_train,\n",
        "        extra_datasets=extra_datasets,\n",
        "        sampler_config=basic_sampler_cfg,\n",
        "        y_train=y_train,\n",
        "        verbose=False\n",
        "    )\n",
        "\n",
        "    if len(extras_basic) == 2:\n",
        "        X_val_ready, X_test_ready = extras_basic\n",
        "    else:\n",
        "        X_val_ready = extras_basic[0]\n",
        "        X_test_ready = extras_basic[0]\n",
        "\n",
        "    poly_added = 0\n",
        "    selector_summary = 'none'\n",
        "    manual_features = 'none'\n",
        "\n",
        "    if options.get('use_advanced'):\n",
        "        X_train_ready, extras_adv = create_advanced_features(\n",
        "            X_train_ready,\n",
        "            extra_datasets=[X_val_ready, X_test_ready],\n",
        "            advanced_config=options.get('advanced_config'),\n",
        "            verbose=False\n",
        "        )\n",
        "        X_val_ready, X_test_ready = extras_adv\n",
        "        manual_features = 'advanced19'\n",
        "\n",
        "    poly_cfg = options.get('poly')\n",
        "    if poly_cfg:\n",
        "        X_train_ready, extras_poly, new_cols = apply_polynomial_features(\n",
        "            X_train_ready,\n",
        "            extra_datasets=[X_val_ready, X_test_ready],\n",
        "            poly_config=poly_cfg,\n",
        "            verbose=False\n",
        "        )\n",
        "        X_val_ready, X_test_ready = extras_poly\n",
        "        poly_added = len(new_cols)\n",
        "\n",
        "    selector_cfg = options.get('feature_selector')\n",
        "    if selector_cfg and y_train_ready is not None:\n",
        "        X_train_ready, extras_sel, _, selector_summary = apply_feature_selector(\n",
        "            X_train_ready,\n",
        "            np.asarray(y_train_ready),\n",
        "            extra_datasets=[X_val_ready, X_test_ready],\n",
        "            selector_config=selector_cfg,\n",
        "            verbose=False\n",
        "        )\n",
        "        X_val_ready, X_test_ready = extras_sel\n",
        "\n",
        "    if advanced_sampler_cfg and y_train_ready is not None:\n",
        "        sampler = get_sampler(advanced_sampler_cfg)\n",
        "        X_resampled, y_resampled = sampler.fit_resample(X_train_ready.values, np.asarray(y_train_ready)) # type: ignore\n",
        "        X_train_ready = pd.DataFrame(X_resampled, columns=X_train_ready.columns)\n",
        "        y_train_ready = y_resampled\n",
        "        adv_sampler = advanced_sampler_cfg.get('name', 'unknown')\n",
        "    else:\n",
        "        adv_sampler = 'none'\n",
        "\n",
        "    prep_meta = {\n",
        "        'feature_pipeline': pipeline_name,\n",
        "        'family': config.get('family', 'linear'),\n",
        "        'description': config.get('description', ''),\n",
        "        'poly_added': poly_added,\n",
        "        'selector': selector_summary,\n",
        "        'basic_sampler': basic_sampler_cfg['name'] if basic_sampler_cfg else 'none',\n",
        "        'advanced_sampler': adv_sampler,\n",
        "        'sampler_summary': f\"basic:{basic_sampler_cfg['name'] if basic_sampler_cfg else 'none'} -> advanced:{adv_sampler}\",\n",
        "        'cv_mode': config.get('cv_mode', 'kfold'),\n",
        "        'encoder': 'onehot',\n",
        "        'scaler': 'standard',\n",
        "        'manual_features': manual_features\n",
        "    }\n",
        "\n",
        "    return X_train_ready, X_val_ready, X_test_ready, np.asarray(y_train_ready), prep_meta\n",
        "\n",
        "\n",
        "def prepare_tree_features(pipeline_name, config, X_train, X_val, X_test, y_train):\n",
        "    options = dict(config.get('options', {}))\n",
        "    encoding = options.get('encoding', 'label')\n",
        "    use_smote = bool(options.get('use_smote', False)) and HAS_IMB\n",
        "    sampler_cfg = options.get('sampler', {'name': 'smote'})\n",
        "    cat_cols = [col for col in NOMINAL_CATEGORICALS if col in X_train.columns]\n",
        "\n",
        "    X_train_proc = X_train.copy()\n",
        "    X_val_proc = X_val.copy()\n",
        "    X_test_proc = X_test.copy()\n",
        "    encoder_label = 'none'\n",
        "\n",
        "    if cat_cols:\n",
        "        if encoding == 'label':\n",
        "            ordinal = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
        "            X_train_proc[cat_cols] = ordinal.fit_transform(X_train_proc[cat_cols])\n",
        "            X_val_proc[cat_cols] = ordinal.transform(X_val_proc[cat_cols])\n",
        "            X_test_proc[cat_cols] = ordinal.transform(X_test_proc[cat_cols])\n",
        "            encoder_label = 'label'\n",
        "        elif encoding == 'onehot':\n",
        "            oh = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
        "            oh.fit(X_train_proc[cat_cols])\n",
        "\n",
        "            def _transform(dataset):\n",
        "                encoded = oh.transform(dataset[cat_cols])\n",
        "                encoded_df = pd.DataFrame(encoded, columns=oh.get_feature_names_out(cat_cols), index=dataset.index) # type: ignore\n",
        "                base = dataset.drop(columns=cat_cols)\n",
        "                return pd.concat([base, encoded_df], axis=1)\n",
        "\n",
        "            X_train_proc = _transform(X_train_proc)\n",
        "            X_val_proc = _transform(X_val_proc)\n",
        "            X_test_proc = _transform(X_test_proc)\n",
        "            encoder_label = 'onehot'\n",
        "        elif encoding == 'target':\n",
        "            if y_train is None:\n",
        "                raise ValueError(\"Target encoding requires y_train\")\n",
        "            y_series = pd.Series(np.asarray(y_train), index=X_train_proc.index)\n",
        "            global_mean = float(y_series.mean())\n",
        "            mappings = {col: y_series.groupby(X_train_proc[col]).mean() for col in cat_cols}\n",
        "\n",
        "            def _apply_target(dataset):\n",
        "                base = dataset.drop(columns=cat_cols).copy()\n",
        "                for col in cat_cols:\n",
        "                    base[f\"{col}_target\"] = dataset[col].map(mappings[col]).fillna(global_mean)\n",
        "                return base\n",
        "\n",
        "            X_train_proc = _apply_target(X_train_proc)\n",
        "            X_val_proc = _apply_target(X_val_proc)\n",
        "            X_test_proc = _apply_target(X_test_proc)\n",
        "            encoder_label = 'target'\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown tree encoding: {encoding}\")\n",
        "\n",
        "    y_ready = np.asarray(y_train) if y_train is not None else None\n",
        "    sampler_summary = 'none'\n",
        "    if use_smote and y_ready is not None:\n",
        "        sampler = get_sampler(sampler_cfg)\n",
        "        X_resampled, y_resampled = sampler.fit_resample(X_train_proc.values, y_ready) # type: ignore\n",
        "        X_train_proc = pd.DataFrame(X_resampled, columns=X_train_proc.columns)\n",
        "        y_ready = y_resampled\n",
        "        sampler_summary = sampler_cfg.get('name', 'smote')\n",
        "\n",
        "    prep_meta = {\n",
        "        'feature_pipeline': pipeline_name,\n",
        "        'family': config.get('family', 'tree'),\n",
        "        'description': config.get('description', ''),\n",
        "        'poly_added': 0,\n",
        "        'selector': 'none',\n",
        "        'basic_sampler': sampler_summary,\n",
        "        'advanced_sampler': 'none',\n",
        "        'sampler_summary': sampler_summary,\n",
        "        'cv_mode': config.get('cv_mode', 'kfold'),\n",
        "        'encoder': encoder_label,\n",
        "        'scaler': 'none',\n",
        "        'manual_features': 'none'\n",
        "    }\n",
        "\n",
        "    return X_train_proc, X_val_proc, X_test_proc, y_ready, prep_meta\n",
        "\n",
        "\n",
        "def prepare_neural_features(pipeline_name, config, X_train, X_val, X_test, y_train):\n",
        "    options = dict(config.get('options', {}))\n",
        "    scaler_name = options.get('scaler', 'standard').lower()\n",
        "    use_smote = bool(options.get('use_smote', False)) and HAS_IMB\n",
        "    sampler_cfg = options.get('sampler', {'name': 'smote'})\n",
        "    cat_cols = [col for col in NOMINAL_CATEGORICALS if col in X_train.columns]\n",
        "\n",
        "    encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
        "    encoder.fit(X_train[cat_cols])\n",
        "\n",
        "    def _encode(dataset):\n",
        "        encoded = encoder.transform(dataset[cat_cols])\n",
        "        encoded_df = pd.DataFrame(encoded, columns=encoder.get_feature_names_out(cat_cols), index=dataset.index) # type: ignore\n",
        "        base = dataset.drop(columns=cat_cols)\n",
        "        return pd.concat([base, encoded_df], axis=1)\n",
        "\n",
        "    X_train_encoded = _encode(X_train)\n",
        "    X_val_encoded = _encode(X_val)\n",
        "    X_test_encoded = _encode(X_test)\n",
        "\n",
        "    scaler = StandardScaler() if scaler_name == 'standard' else MinMaxScaler()\n",
        "    feature_columns = X_train_encoded.columns\n",
        "    X_train_scaled = pd.DataFrame(\n",
        "        scaler.fit_transform(X_train_encoded),\n",
        "        columns=feature_columns,\n",
        "        index=X_train_encoded.index\n",
        "    )\n",
        "    X_val_scaled = pd.DataFrame(\n",
        "        scaler.transform(X_val_encoded),\n",
        "        columns=feature_columns,\n",
        "        index=X_val_encoded.index\n",
        "    )\n",
        "    X_test_scaled = pd.DataFrame(\n",
        "        scaler.transform(X_test_encoded),\n",
        "        columns=feature_columns,\n",
        "        index=X_test_encoded.index\n",
        "    )\n",
        "\n",
        "    y_ready = np.asarray(y_train) if y_train is not None else None\n",
        "    sampler_summary = 'none'\n",
        "    if use_smote and y_ready is not None:\n",
        "        sampler = get_sampler(sampler_cfg)\n",
        "        X_resampled, y_resampled = sampler.fit_resample(X_train_scaled.values, y_ready) # type: ignore\n",
        "        X_train_scaled = pd.DataFrame(X_resampled, columns=feature_columns)\n",
        "        y_ready = y_resampled\n",
        "        sampler_summary = sampler_cfg.get('name', 'smote')\n",
        "\n",
        "    prep_meta = {\n",
        "        'feature_pipeline': pipeline_name,\n",
        "        'family': config.get('family', 'neural'),\n",
        "        'description': config.get('description', ''),\n",
        "        'poly_added': 0,\n",
        "        'selector': 'none',\n",
        "        'basic_sampler': sampler_summary,\n",
        "        'advanced_sampler': 'none',\n",
        "        'sampler_summary': sampler_summary,\n",
        "        'cv_mode': config.get('cv_mode', 'kfold'),\n",
        "        'encoder': 'onehot',\n",
        "        'scaler': 'minmax' if scaler_name == 'minmax' else 'standard',\n",
        "        'manual_features': 'none'\n",
        "    }\n",
        "\n",
        "    return X_train_scaled, X_val_scaled, X_test_scaled, y_ready, prep_meta\n",
        "\n",
        "\n",
        "def prepare_feature_set(pipeline_name, config, X_train, X_val, X_test, y_train):\n",
        "    family = config.get('family', 'linear')\n",
        "    if family == 'linear':\n",
        "        return prepare_linear_features(pipeline_name, config, X_train, X_val, X_test, y_train)\n",
        "    if family == 'tree':\n",
        "        return prepare_tree_features(pipeline_name, config, X_train, X_val, X_test, y_train)\n",
        "    if family == 'neural':\n",
        "        return prepare_neural_features(pipeline_name, config, X_train, X_val, X_test, y_train)\n",
        "    raise ValueError(f\"Unknown model family '{family}' for pipeline '{pipeline_name}'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "print(\"\\n[6/8] Configuring Model Grids...\")\n",
        "\n",
        "LR_CONFIGS = [\n",
        "    {'C': 0.3, 'penalty': 'l2', 'solver': 'lbfgs', 'max_iter': 3000},\n",
        "    {'C': 1.0, 'penalty': 'l2', 'solver': 'lbfgs', 'max_iter': 4000},\n",
        "    {'C': 2.0, 'penalty': 'l2', 'solver': 'lbfgs', 'class_weight': 'balanced', 'max_iter': 4000},\n",
        "    {'C': 0.5, 'penalty': 'l1', 'solver': 'liblinear', 'max_iter': 3500},\n",
        "    {'C': 1.0, 'penalty': 'elasticnet', 'solver': 'saga', 'l1_ratio': 0.3, 'max_iter': 4500}\n",
        "]\n",
        "\n",
        "SVC_CONFIGS = [\n",
        "    {'C': 0.5, 'kernel': 'rbf', 'gamma': 'scale', 'probability': True},\n",
        "    {'C': 2.0, 'kernel': 'rbf', 'gamma': 0.05, 'probability': True, 'class_weight': 'balanced'}\n",
        "]\n",
        "\n",
        "RF_CONFIGS = [\n",
        "    {'n_estimators': 600, 'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 2},\n",
        "    {'n_estimators': 900, 'max_depth': 12, 'max_features': 0.6, 'min_samples_leaf': 2},\n",
        "    {'n_estimators': 1100, 'max_depth': 14, 'max_features': 0.5, 'min_samples_leaf': 2},\n",
        "    {'n_estimators': 900, 'max_depth': None, 'max_features': 0.7, 'min_samples_leaf': 3}\n",
        "]\n",
        "\n",
        "ET_CONFIGS = [\n",
        "    {'n_estimators': 500, 'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 2},\n",
        "    {'n_estimators': 800, 'max_depth': 12, 'max_features': 0.6, 'min_samples_leaf': 1},\n",
        "    {'n_estimators': 1000, 'max_depth': None, 'max_features': 0.5, 'min_samples_leaf': 2}\n",
        "]\n",
        "\n",
        "GB_CONFIGS = [\n",
        "    {'n_estimators': 500, 'learning_rate': 0.05, 'max_depth': 3, 'subsample': 0.9},\n",
        "    {'n_estimators': 800, 'learning_rate': 0.03, 'max_depth': 4, 'subsample': 0.85},\n",
        "    {'n_estimators': 900, 'learning_rate': 0.025, 'max_depth': 4, 'subsample': 0.85},\n",
        "    {'n_estimators': 1100, 'learning_rate': 0.02, 'max_depth': 5, 'subsample': 0.8}\n",
        "]\n",
        "\n",
        "HGB_CONFIGS = [\n",
        "    {'learning_rate': 0.05, 'max_depth': 6, 'max_iter': 600, 'l2_regularization': 0.1},\n",
        "    {'learning_rate': 0.04, 'max_depth': 8, 'max_iter': 800, 'l2_regularization': 0.1},\n",
        "    {'learning_rate': 0.03, 'max_depth': 10, 'max_iter': 900, 'l2_regularization': 0.05},\n",
        "    {'learning_rate': 0.02, 'max_depth': 12, 'max_iter': 1100, 'l2_regularization': 0.05}\n",
        "]\n",
        "\n",
        "XGB_CONFIGS = [\n",
        "    {'n_estimators': 1000, 'learning_rate': 0.05, 'max_depth': 8, 'subsample': 0.9, 'colsample_bytree': 0.75, 'reg_lambda': 1.0, 'scale_pos_weight': 1.0},\n",
        "    {'n_estimators': 1400, 'learning_rate': 0.04, 'max_depth': 9, 'subsample': 0.85, 'colsample_bytree': 0.8, 'reg_lambda': 1.2, 'scale_pos_weight': 1.2},\n",
        "    {'n_estimators': 1600, 'learning_rate': 0.03, 'max_depth': 10, 'subsample': 0.85, 'colsample_bytree': 0.8, 'reg_lambda': 1.5, 'scale_pos_weight': 1.5},\n",
        "    {'n_estimators': 1800, 'learning_rate': 0.025, 'max_depth': 11, 'subsample': 0.8, 'colsample_bytree': 0.8, 'reg_lambda': 1.8, 'scale_pos_weight': 1.8}\n",
        "]\n",
        "\n",
        "LGB_CONFIGS = [\n",
        "    {'n_estimators': 1000, 'learning_rate': 0.05, 'num_leaves': 64, 'subsample': 0.9, 'colsample_bytree': 0.8, 'reg_lambda': 0.0},\n",
        "    {'n_estimators': 1300, 'learning_rate': 0.035, 'num_leaves': 80, 'subsample': 0.85, 'colsample_bytree': 0.8, 'reg_lambda': 0.3},\n",
        "    {'n_estimators': 1500, 'learning_rate': 0.03, 'num_leaves': 96, 'subsample': 0.85, 'colsample_bytree': 0.85, 'reg_lambda': 0.5},\n",
        "    {'n_estimators': 1700, 'learning_rate': 0.025, 'num_leaves': 120, 'subsample': 0.8, 'colsample_bytree': 0.85, 'reg_lambda': 0.8}\n",
        "]\n",
        "\n",
        "CAT_CONFIGS = [\n",
        "    {'iterations': 1000, 'learning_rate': 0.05, 'depth': 7, 'l2_leaf_reg': 3.0},\n",
        "    {'iterations': 1300, 'learning_rate': 0.04, 'depth': 8, 'l2_leaf_reg': 2.5},\n",
        "    {'iterations': 1500, 'learning_rate': 0.035, 'depth': 9, 'l2_leaf_reg': 2.0},\n",
        "    {'iterations': 1700, 'learning_rate': 0.03, 'depth': 9, 'l2_leaf_reg': 2.5}\n",
        "]\n",
        "\n",
        "MLP_CONFIGS = [\n",
        "    {'hidden_layer_sizes': (512,), 'alpha': 1e-4, 'learning_rate_init': 0.001, 'max_iter': 1500, 'early_stopping': True, 'n_iter_no_change': 30},\n",
        "    {'hidden_layer_sizes': (256,), 'alpha': 4e-4, 'learning_rate_init': 0.001, 'max_iter': 1500, 'early_stopping': True, 'n_iter_no_change': 30},\n",
        "    {'hidden_layer_sizes': (512, 256), 'alpha': 1e-4, 'learning_rate_init': 0.0008, 'max_iter': 1700, 'early_stopping': True, 'n_iter_no_change': 25},\n",
        "    {'hidden_layer_sizes': (256, 128), 'alpha': 2e-4, 'learning_rate_init': 0.0008, 'max_iter': 1700, 'early_stopping': True, 'n_iter_no_change': 25},\n",
        "    {'hidden_layer_sizes': (512, 256, 128), 'alpha': 7e-5, 'learning_rate_init': 0.0005, 'max_iter': 1800, 'early_stopping': True, 'n_iter_no_change': 30},\n",
        "    {'hidden_layer_sizes': (256, 128, 64), 'alpha': 1.5e-4, 'learning_rate_init': 0.0005, 'max_iter': 1800, 'early_stopping': True, 'n_iter_no_change': 30}\n",
        "]\n",
        "\n",
        "MODEL_CONFIGS = [\n",
        "    ('LR', LogisticRegression, LR_CONFIGS),\n",
        "    ('RF', RandomForestClassifier, RF_CONFIGS),\n",
        "    ('ET', ExtraTreesClassifier, ET_CONFIGS),\n",
        "    ('GB', GradientBoostingClassifier, GB_CONFIGS),\n",
        "    ('HGB', HistGradientBoostingClassifier, HGB_CONFIGS),\n",
        "    ('MLP', MLPClassifier, MLP_CONFIGS),\n",
        "    ('SVC', SVC, SVC_CONFIGS)\n",
        "]\n",
        "\n",
        "if HAS_IMB and BalancedRandomForestClassifier is not None:\n",
        "    BRF_CONFIGS = [\n",
        "        {'n_estimators': 600, 'max_depth': 10, 'max_features': 'sqrt'},\n",
        "        {'n_estimators': 900, 'max_depth': 12, 'max_features': 0.6},\n",
        "        {'n_estimators': 1100, 'max_depth': 14, 'max_features': 0.5}\n",
        "    ]\n",
        "    MODEL_CONFIGS.append(('BRF', BalancedRandomForestClassifier, BRF_CONFIGS))\n",
        "\n",
        "if HAS_XGB and xgb is not None:\n",
        "    MODEL_CONFIGS.append(('XGB', xgb.XGBClassifier, XGB_CONFIGS))\n",
        "\n",
        "if HAS_LGB and lgb is not None:\n",
        "    MODEL_CONFIGS.append(('LGB', lgb.LGBMClassifier, LGB_CONFIGS))\n",
        "\n",
        "if HAS_CAT and cb is not None:\n",
        "    MODEL_CONFIGS.append(('CAT', cb.CatBoostClassifier, CAT_CONFIGS))\n",
        "\n",
        "family_pipeline_counts = {family: sum(1 for cfg in FEATURE_PIPELINES.values() if cfg['family'] == family) for family in MODEL_FAMILIES}\n",
        "per_seed_experiments = 0\n",
        "for model_name, _, configs in MODEL_CONFIGS:\n",
        "    family = MODEL_TO_FAMILY.get(model_name)\n",
        "    if family is None:\n",
        "        continue\n",
        "    per_seed_experiments += len(configs) * family_pipeline_counts.get(family, 0)\n",
        "\n",
        "print(f\"  ✓ Model families: {', '.join(MODEL_FAMILIES.keys())}\")\n",
        "print(f\"  ✓ Total pipelines: {len(FEATURE_PIPELINES)}\")\n",
        "print(f\"  ✓ Total model configs: {sum(len(cfgs) for _, _, cfgs in MODEL_CONFIGS)}\")\n",
        "print(f\"  ✓ Base experiments per seed: {per_seed_experiments}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# ============================================================================\n",
        "# Artifact Utilities\n",
        "# ============================================================================\n",
        "print(\"\\n[6b/8] Preparing artifact utility helpers...\")\n",
        "\n",
        "\n",
        "def export_family_tables(results_df: pd.DataFrame, top_n: int = 10) -> None:\n",
        "    if len(results_df) == 0:\n",
        "        print(\"  [WARN] No results to export for families\")\n",
        "        return\n",
        "    summary_rows = []\n",
        "    for family, dirs in FAMILY_ARTIFACTS.items():\n",
        "        fam_df = results_df[results_df['family'] == family]\n",
        "        if fam_df.empty:\n",
        "            continue\n",
        "        all_path = dirs['data'] / 'all_results.csv'\n",
        "        top_path = dirs['data'] / f'top{top_n}_results.csv'\n",
        "        fam_df.to_csv(all_path, index=False)\n",
        "        fam_df.head(top_n).to_csv(top_path, index=False)\n",
        "        summary_rows.append({\n",
        "            'family': family,\n",
        "            'all_results_path': str(all_path),\n",
        "            'top_results_path': str(top_path),\n",
        "            'experiment_count': len(fam_df)\n",
        "        })\n",
        "    if summary_rows:\n",
        "        pd.DataFrame(summary_rows).to_csv(SUMMARY_DIR / 'family_data_manifest.csv', index=False)\n",
        "        print(f\"  ✓ Stored family tables for {len(summary_rows)} families\")\n",
        "    else:\n",
        "        print(\"  [WARN] No family tables created (no results)\")\n",
        "\n",
        "\n",
        "def plot_family_val_vs_test(results_df: pd.DataFrame, top_n: int = 10) -> None:\n",
        "    if len(results_df) == 0:\n",
        "        return\n",
        "    families = list(MODEL_FAMILIES.keys())\n",
        "    fig, axes = plt.subplots(1, len(families), figsize=(6 * len(families), 6), sharey=True)\n",
        "    if len(families) == 1:\n",
        "        axes = [axes]\n",
        "    for ax, family in zip(axes, families):\n",
        "        fam_df = results_df[results_df['family'] == family].sort_values('test_auc', ascending=False).head(top_n)\n",
        "        if fam_df.empty:\n",
        "            ax.set_title(f\"{family.title()} (no runs)\")\n",
        "            continue\n",
        "        idx = np.arange(len(fam_df))\n",
        "        width = 0.35\n",
        "        ax.bar(idx - width / 2, fam_df['cv_auc_mean'], width, label='CV AUC', color='#4c72b0')\n",
        "        ax.bar(idx + width / 2, fam_df['test_auc'], width, label='Test AUC', color='#dd8452')\n",
        "        ax.set_xticks(idx)\n",
        "        ax.set_xticklabels(fam_df['model'], rotation=45, ha='right')\n",
        "        ax.set_ylabel('AUC')\n",
        "        ax.set_ylim(0.7, 0.92)\n",
        "        ax.set_title(f\"{family.title()} top{len(fam_df)}\")\n",
        "        ax.grid(axis='y', linestyle='--', alpha=0.4)\n",
        "    axes[0].legend()\n",
        "    fig.tight_layout()\n",
        "    fig_path = GLOBAL_FIG_DIR / 'family_val_vs_test.png'\n",
        "    fig.savefig(fig_path, dpi=200, bbox_inches='tight')\n",
        "    plt.close(fig)\n",
        "    for family, dirs in FAMILY_ARTIFACTS.items():\n",
        "        target = dirs['figures'] / 'family_val_vs_test.png'\n",
        "        try:\n",
        "            shutil.copy(fig_path, target)\n",
        "        except Exception:\n",
        "            pass\n",
        "    print(f\"  ✓ Saved family CV/Test comparison: {fig_path}\")\n",
        "\n",
        "\n",
        "def plot_pipeline_contrast(results_df: pd.DataFrame) -> None:\n",
        "    if len(results_df) == 0:\n",
        "        return\n",
        "    pipeline_stats = results_df.groupby('feature_pipeline')[['cv_auc_mean', 'test_auc']].mean().sort_values('test_auc', ascending=False)\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    idx = np.arange(len(pipeline_stats))\n",
        "    width = 0.35\n",
        "    ax.bar(idx - width / 2, pipeline_stats['cv_auc_mean'], width, label='CV AUC', color='#55a868')\n",
        "    ax.bar(idx + width / 2, pipeline_stats['test_auc'], width, label='Test AUC', color='#c44e52')\n",
        "    ax.set_xticks(idx)\n",
        "    ax.set_xticklabels(pipeline_stats.index, rotation=45, ha='right')\n",
        "    ax.set_ylabel('AUC')\n",
        "    ax.set_title('Average performance by feature pipeline')\n",
        "    ax.legend()\n",
        "    ax.grid(axis='y', linestyle='--', alpha=0.4)\n",
        "    fig.tight_layout()\n",
        "    fig_path = GLOBAL_FIG_DIR / 'pipeline_val_test_comparison.png'\n",
        "    fig.savefig(fig_path, dpi=200, bbox_inches='tight')\n",
        "    plt.close(fig)\n",
        "    print(f\"  ✓ Saved pipeline comparison: {fig_path}\")\n",
        "\n",
        "\n",
        "def plot_model_scatter(results_df: pd.DataFrame) -> None:\n",
        "    if len(results_df) == 0:\n",
        "        return\n",
        "    fig, ax = plt.subplots(figsize=(8, 6))\n",
        "    fam_to_idx = {family: idx for idx, family in enumerate(MODEL_FAMILIES.keys())}\n",
        "    colors = [fam_to_idx.get(fam, -1) for fam in results_df['family']]\n",
        "    scatter = ax.scatter(results_df['cv_auc_mean'], results_df['test_auc'], c=colors, cmap='tab10', alpha=0.65, s=45)\n",
        "    ax.plot([0, 1], [0, 1], linestyle='--', color='gray', alpha=0.3)\n",
        "    ax.set_xlim(0.74, 0.83)\n",
        "    ax.set_ylim(0.8, 0.89)\n",
        "    ax.set_xlabel('CV AUC')\n",
        "    ax.set_ylabel('Test AUC')\n",
        "    ax.set_title('CV vs Test AUC (colored by family)')\n",
        "    handles = []\n",
        "    labels = []\n",
        "    for fam, idx in fam_to_idx.items():\n",
        "        handles.append(plt.Line2D([], [], marker='o', linestyle='', color=scatter.cmap(scatter.norm(idx)))) # type: ignore\n",
        "        labels.append(fam.title())\n",
        "    ax.legend(handles, labels, title='Family', loc='lower right')\n",
        "    ax.grid(alpha=0.4)\n",
        "    fig.tight_layout()\n",
        "    fig_path = GLOBAL_FIG_DIR / 'cv_vs_test_scatter.png'\n",
        "    fig.savefig(fig_path, dpi=200, bbox_inches='tight')\n",
        "    plt.close(fig)\n",
        "    print(f\"  ✓ Saved CV/Test scatter: {fig_path}\")\n",
        "\n",
        "\n",
        "def persist_top_models(results_df: pd.DataFrame, top_n: int = 10) -> None:\n",
        "    if len(results_df) == 0:\n",
        "        print(\"  [WARN] No models available for persistence\")\n",
        "        return\n",
        "    saved_rows = []\n",
        "    for family, dirs in FAMILY_ARTIFACTS.items():\n",
        "        fam_df = results_df[results_df['family'] == family].sort_values('test_auc', ascending=False).head(top_n)\n",
        "        if fam_df.empty:\n",
        "            continue\n",
        "        for row in fam_df.itertuples():\n",
        "            params = row.hyperparams\n",
        "            if isinstance(params, str):\n",
        "                try:\n",
        "                    params = ast.literal_eval(params)\n",
        "                except Exception:\n",
        "                    params = {}\n",
        "            model_cls = MODEL_CLASS_LOOKUP.get(row.model)\n",
        "            pipeline_cfg = FEATURE_PIPELINES.get(row.feature_pipeline)\n",
        "            if model_cls is None or pipeline_cfg is None:\n",
        "                continue\n",
        "            np.random.seed(getattr(row, 'random_seed', RANDOM_STATE))\n",
        "            prep_tuple = prepare_feature_set(\n",
        "                row.feature_pipeline,\n",
        "                pipeline_cfg,\n",
        "                X_train_raw,\n",
        "                X_train_raw,\n",
        "                X_test_raw,\n",
        "                y_train_raw\n",
        "            )\n",
        "            if len(prep_tuple) == 5:\n",
        "                X_ready, _, _, y_ready, prep_meta = prep_tuple\n",
        "            else:\n",
        "                X_ready, _, _, y_ready, prep_meta = prep_tuple[:5] # type: ignore\n",
        "            model = model_cls(**params)\n",
        "            model.fit(X_ready, y_ready)\n",
        "            payload = {\n",
        "                'model': model,\n",
        "                'model_name': row.model,\n",
        "                'family': family,\n",
        "                'feature_pipeline': row.feature_pipeline,\n",
        "                'feature_columns': list(X_ready.columns),\n",
        "                'hyperparams': params,\n",
        "                'training_seed': getattr(row, 'random_seed', RANDOM_STATE),\n",
        "                'cv_auc_mean': row.cv_auc_mean,\n",
        "                'cv_auc_std': row.cv_auc_std,\n",
        "                'test_auc': row.test_auc,\n",
        "                'feature_count': row.features_count,\n",
        "                'prep_meta': prep_meta\n",
        "            }\n",
        "            model_path = dirs['models'] / f\"{row.exp_name}.joblib\"\n",
        "            joblib.dump(payload, model_path)\n",
        "            saved_rows.append({\n",
        "                'family': family,\n",
        "                'exp_name': row.exp_name,\n",
        "                'model_path': str(model_path),\n",
        "                'cv_auc_mean': row.cv_auc_mean,\n",
        "                'test_auc': row.test_auc\n",
        "            })\n",
        "    if saved_rows:\n",
        "        manifest_path = SUMMARY_DIR / 'top_model_manifest.csv'\n",
        "        pd.DataFrame(saved_rows).to_csv(manifest_path, index=False)\n",
        "        print(f\"  ✓ Persisted {len(saved_rows)} trained models -> {manifest_path}\")\n",
        "    else:\n",
        "        print(\"  [WARN] No models persisted (check filters)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "experiment_count = 0\n",
        "\n",
        "MODEL_BASE_PARAMS = {\n",
        "    'LR': lambda seed: {'random_state': seed},\n",
        "    'RF': lambda seed: {'random_state': seed, 'n_jobs': -1},\n",
        "    'ET': lambda seed: {'random_state': seed, 'n_jobs': -1},\n",
        "    'GB': lambda seed: {'random_state': seed},\n",
        "    'HGB': lambda seed: {'random_state': seed},\n",
        "    'MLP': lambda seed: {'random_state': seed},\n",
        "    'SVC': lambda seed: {'random_state': seed, 'cache_size': 1000},\n",
        "    'BRF': lambda seed: {'random_state': seed, 'n_jobs': -1},\n",
        "    'XGB': lambda seed: {'random_state': seed, 'eval_metric': 'auc', 'n_jobs': -1, 'tree_method': 'hist', 'verbosity': 0},\n",
        "    'LGB': lambda seed: {'random_state': seed, 'n_jobs': -1, 'verbosity': -1},\n",
        "    'CAT': lambda seed: {'random_seed': seed,'verbose': 0,'eval_metric': 'AUC','train_dir': None}\n",
        "}\n",
        "\n",
        "MODEL_CLASS_LOOKUP = {name: cls for name, cls, _ in MODEL_CONFIGS}\n",
        "\n",
        "for seed in RANDOM_SEEDS:\n",
        "    print(f\"\\n{'#'*80}\")\n",
        "    print(f\"GLOBAL SEED: {seed}\")\n",
        "    print(f\"{'#'*80}\")\n",
        "\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    for pipeline_name, pipeline_config in FEATURE_PIPELINES.items():\n",
        "        family = pipeline_config.get('family')\n",
        "        eligible_models = [entry for entry in MODEL_CONFIGS if MODEL_TO_FAMILY.get(entry[0]) == family]\n",
        "        if not eligible_models:\n",
        "            continue\n",
        "\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"PIPELINE: {pipeline_name.upper()} | Family: {family} | CV: {pipeline_config.get('cv_mode', 'kfold')}\")\n",
        "        print(pipeline_config.get('description', ''))\n",
        "        print(f\"{'='*80}\")\n",
        "\n",
        "        for model_name, model_class, configs in eligible_models:\n",
        "            print(f\"\\n  [{model_name}] Testing {len(configs)} configurations...\")\n",
        "\n",
        "            for idx, config in enumerate(configs, 1):\n",
        "                experiment_count += 1\n",
        "\n",
        "                base_params = MODEL_BASE_PARAMS.get(model_name, lambda s: {})(seed)\n",
        "                base_params.setdefault('random_state', seed)\n",
        "                if model_name in ['RF', 'ET', 'BRF', 'XGB', 'LGB']:\n",
        "                    base_params.setdefault('n_jobs', -1)\n",
        "                if model_name == 'CAT':\n",
        "                    base_params.pop('random_state', None)\n",
        "\n",
        "                merged_params = {**base_params, **config}\n",
        "\n",
        "                try:\n",
        "                    model = model_class(**merged_params)\n",
        "                except TypeError as e:\n",
        "                    print(f\"    Config {idx} skipped (param error): {e}\")\n",
        "                    continue\n",
        "\n",
        "                try:\n",
        "                    cv_mean, cv_std, test_auc, _, feature_count, prep_meta = evaluate_with_cv(\n",
        "                        model,\n",
        "                        X_train_raw,\n",
        "                        y_train_raw,\n",
        "                        X_test_raw,\n",
        "                        y_test_raw,\n",
        "                        feature_config=pipeline_config,\n",
        "                        pipeline_key=pipeline_name\n",
        "                    )\n",
        "\n",
        "                    metadata = {\n",
        "                        'feature_pipeline': pipeline_name,\n",
        "                        'family': family,\n",
        "                        'feature_description': pipeline_config.get('description', ''),\n",
        "                        'random_seed': seed,\n",
        "                        'cv_mode': pipeline_config.get('cv_mode', 'kfold'),\n",
        "                        'sampler_summary': prep_meta.get('sampler_summary', 'none'),\n",
        "                        'encoder': prep_meta.get('encoder', 'onehot'),\n",
        "                        'scaler': prep_meta.get('scaler', 'standard'),\n",
        "                        'manual_features': prep_meta.get('manual_features', 'none'),\n",
        "                        'poly_added': prep_meta.get('poly_added', 0),\n",
        "                        'selector': prep_meta.get('selector', 'none'),\n",
        "                        'basic_sampler': prep_meta.get('basic_sampler', 'none'),\n",
        "                        'advanced_sampler': prep_meta.get('advanced_sampler', 'none')\n",
        "                    }\n",
        "\n",
        "                    exp_name = f\"seed{seed}_{pipeline_name}_{model_name}_v{idx}\"\n",
        "\n",
        "                    log_experiment(\n",
        "                        exp_name=exp_name,\n",
        "                        model_name=model_name,\n",
        "                        cv_auc_mean=cv_mean,\n",
        "                        cv_auc_std=cv_std,\n",
        "                        test_auc=test_auc,\n",
        "                        features_used=feature_count,\n",
        "                        hyperparams=merged_params,\n",
        "                        notes=pipeline_config.get('description', ''),\n",
        "                        metadata=metadata\n",
        "                    )\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"    Config {idx} failed: {str(e)}\")\n",
        "                    continue\n",
        "\n",
        "            print(f\"  Completed {len(configs)} {model_name} experiments\")\n",
        "\n",
        "print(f\"\\nTotal experiments scheduled: {experiment_count}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# ============================================================================\n",
        "# Results Consolidation and Reporting\n",
        "# ============================================================================\n",
        "print(\"\\n[8/8] Results Analysis and Ranking...\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "results_df = get_results_df()\n",
        "\n",
        "if len(results_df) == 0:\n",
        "    print(\"[WARN] No experiments recorded!\")\n",
        "else:\n",
        "    full_path = SUMMARY_DIR / 'experiment_results_fair_full.csv'\n",
        "    results_df.to_csv(full_path, index=False)\n",
        "    print(f\"[INFO] Full results saved to: {full_path}\")\n",
        "\n",
        "    family_top = results_df.sort_values('test_auc', ascending=False).groupby('family', group_keys=False).head(10)\n",
        "    best_family_path = SUMMARY_DIR / 'best_by_family.csv'\n",
        "    family_top.to_csv(best_family_path, index=False)\n",
        "    print(f\"[INFO] Per-family Top 10 saved to: {best_family_path}\")\n",
        "\n",
        "    overall_top = results_df.head(50)\n",
        "    overall_path = SUMMARY_DIR / 'overall_ranking.csv'\n",
        "    overall_top.to_csv(overall_path, index=False)\n",
        "    print(f\"[INFO] Overall leaderboard saved to: {overall_path}\")\n",
        "\n",
        "    champion = results_df.iloc[0]\n",
        "    print(\"\\n========== OVERALL CHAMPION ==========\")\n",
        "    print(f\"Winner: {champion['model']} | Pipeline: {champion['family']}::{champion['feature_pipeline']} | Test AUC: {champion['test_auc']:.4f}\")\n",
        "\n",
        "    print(\"\\n========== FAMILY CHAMPIONS ==========\")\n",
        "    for fam_key in MODEL_FAMILIES.keys():\n",
        "        fam_df = results_df[results_df['family'] == fam_key]\n",
        "        if len(fam_df) == 0:\n",
        "            continue\n",
        "        fam_best = fam_df.iloc[0]\n",
        "        print(f\"{fam_key.capitalize():<7}: {fam_best['model']} ({fam_best['feature_pipeline']})  {fam_best['test_auc']:.4f}\")\n",
        "\n",
        "    counts = results_df['family'].value_counts()\n",
        "    print(\"\\n========== FAIRNESS CHECK ==========\")\n",
        "    print(f\"✅ Linear: Scaled + manual feature boosts ({counts.get('linear', 0)} exps)\")\n",
        "    print(f\"✅ Tree: Raw signal + minimal encoding ({counts.get('tree', 0)} exps)\")\n",
        "    print(f\"✅ Neural: Scaled raw inputs + early stop ({counts.get('neural', 0)} exps)\")\n",
        "    print(\"✅ Experiments run under seeds 42 & 2025\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*120)\n",
        "    print(\"TOP 25 CONFIGURATIONS BY TEST AUC\")\n",
        "    print(\"=\"*120)\n",
        "    print(f\"{'Rank':<6} {'Exp Name':<34} {'Model':<8} {'Family':<8} {'Pipeline':<16} {'Seed':<6} {'CV AUC':<18} {'Test AUC':<10} {'Features':<10} {'Poly':<6} {'Selector':<12}\")\n",
        "    print(\"-\"*120)\n",
        "\n",
        "    for rank, row in enumerate(results_df.head(25).itertuples(), 1):\n",
        "        cv_str = f\"{row.cv_auc_mean:.4f}±{row.cv_auc_std:.4f}\"\n",
        "        print(f\"{rank:<6} {row.exp_name:<34} {row.model:<8} {getattr(row, 'family', 'na'):<8} {getattr(row, 'feature_pipeline', 'na'):<16} {getattr(row, 'random_seed', 'na')!s:<6} {cv_str:<18} {row.test_auc:<10.4f} {row.features_count:<10} {getattr(row, 'poly_added', 0):<6} {getattr(row, 'selector', 'none'):<12}\")\n",
        "\n",
        "    print(\"-\"*120)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"STATISTICAL SUMMARY\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    baseline = 0.8797\n",
        "    best_row = results_df.iloc[0]\n",
        "\n",
        "    print(f\"\\n[Perf] Overall Performance:\")\n",
        "    print(f\"  Total Experiments: {len(results_df)}\")\n",
        "    print(f\"  Best Test AUC: {results_df['test_auc'].max():.4f}\")\n",
        "    print(f\"  Mean Test AUC: {results_df['test_auc'].mean():.4f}\")\n",
        "    print(f\"  Median Test AUC: {results_df['test_auc'].median():.4f}\")\n",
        "    print(f\"  Std Test AUC: {results_df['test_auc'].std():.4f}\")\n",
        "\n",
        "    print(f\"\\n[Perf] Best Configuration:\")\n",
        "    print(f\"  Name: {best_row['exp_name']}\")\n",
        "    print(f\"  Model: {best_row['model']}\")\n",
        "    print(f\"  CV AUC: {best_row['cv_auc_mean']:.4f} ± {best_row['cv_auc_std']:.4f}\")\n",
        "    print(f\"  Test AUC: {best_row['test_auc']:.4f}\")\n",
        "    print(f\"  Features: {best_row['features_count']}\")\n",
        "    print(f\"  Seed: {best_row.get('random_seed', 'na')}\")\n",
        "    print(f\"  Pipeline: {best_row.get('family', 'na')}::{best_row.get('feature_pipeline', 'na')}\")\n",
        "    print(f\"  Poly added: {best_row.get('poly_added', 0)} | Selector: {best_row.get('selector', 'none')}\")\n",
        "    print(f\"  Hyperparameters: {best_row['hyperparams']}\")\n",
        "\n",
        "    print(f\"\\n[Baseline] vs Baseline (example.ipynb: {baseline:.4f}):\")\n",
        "    improvement = best_row['test_auc'] - baseline\n",
        "    if improvement > 0:\n",
        "        print(f\"  BEAT BASELINE by {improvement:.4f} ({improvement/baseline*100:.2f}%)\")\n",
        "    elif improvement > -0.005:\n",
        "        print(f\"  MATCHED BASELINE (within {abs(improvement):.4f})\")\n",
        "    else:\n",
        "        print(f\"  Below baseline by {abs(improvement):.4f} ({abs(improvement)/baseline*100:.2f}%)\")\n",
        "\n",
        "    print(f\"\\n[Model] Performance by Model Type:\")\n",
        "    model_stats = results_df.groupby('model')['test_auc'].agg(['mean', 'max', 'count']).sort_values('max', ascending=False)\n",
        "    print(\"\\n\" + model_stats.to_string())\n",
        "\n",
        "    print(f\"\\n[Feature] Performance by Feature Pipeline:\")\n",
        "    feat_stats = results_df.groupby('feature_pipeline')['test_auc'].agg(['mean', 'max', 'count']).sort_values('max', ascending=False)\n",
        "    print(\"\\n\" + feat_stats.to_string())\n",
        "\n",
        "    if 'cv_mode' in results_df.columns:\n",
        "        print(f\"\\n[CV] Performance by CV Mode:\")\n",
        "        cv_stats = results_df.groupby('cv_mode')['test_auc'].agg(['mean', 'max', 'count']).sort_values('max', ascending=False)\n",
        "        print(\"\\n\" + cv_stats.to_string())\n",
        "\n",
        "    if 'random_seed' in results_df.columns:\n",
        "        print(f\"\\n[Seed] Distribution by random seed:\")\n",
        "        seed_stats = results_df.groupby('random_seed')['test_auc'].agg(['mean', 'max', 'count']).sort_values('max', ascending=False)\n",
        "        print(\"\\n\" + seed_stats.to_string())\n",
        "\n",
        "    above_baseline = (results_df['test_auc'] >= baseline).sum()\n",
        "    print(f\"\\n[Counts] Configurations beating baseline: {above_baseline}/{len(results_df)} ({above_baseline/len(results_df)*100:.1f}%)\")\n",
        "\n",
        "    # Persist supporting artifacts so reruns are optional\n",
        "    export_family_tables(results_df)\n",
        "    plot_family_val_vs_test(results_df)\n",
        "    plot_pipeline_contrast(results_df)\n",
        "    plot_model_scatter(results_df)\n",
        "    persist_top_models(results_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# ============================================================================\n",
        "# Production Recommendations\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"PRODUCTION DEPLOYMENT RECOMMENDATIONS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "if len(results_df) > 0:\n",
        "    best_config = results_df.iloc[0]\n",
        "\n",
        "    print(f\"\\n[Deploy] Recommended Configuration for Production:\")\n",
        "    print(f\"  Primary Model: {best_config['model']} | Seed: {best_config.get('random_seed', 'na')}\")\n",
        "    print(f\"  Experiment ID: {best_config['exp_name']}\")\n",
        "    print(f\"  Feature Pipeline: {best_config.get('family', 'na')}::{best_config.get('feature_pipeline', 'na')}\")\n",
        "    print(f\"  Expected Performance: {best_config['test_auc']:.4f} AUC\")\n",
        "\n",
        "    print(f\"\\n  Feature Engineering:\")\n",
        "    print(f\"    Pipeline Description: {best_config.get('feature_description', 'na')}\")\n",
        "    print(f\"    Total Features: {best_config['features_count']}\")\n",
        "    print(f\"    Poly Added: {best_config.get('poly_added', 0)} | Selector: {best_config.get('selector', 'none')}\")\n",
        "    print(f\"    Samplers: {best_config.get('sampler_summary', 'none')}\")\n",
        "\n",
        "    print(f\"\\n  Training Strategy:\")\n",
        "    print(f\"    CV Mode: {best_config.get('cv_mode', 'kfold')} | CV Mean: {best_config['cv_auc_mean']:.4f}\")\n",
        "    print(\"    Keep family-specific preprocessing when refitting on full data\")\n",
        "\n",
        "    print(f\"\\n  Monitoring:\")\n",
        "    print(f\"    Track validation AUC weekly and alert if below {best_config['test_auc'] - 0.02:.4f}\")\n",
        "    print(\"    Re-run family search quarterly or with data drift signals\")\n",
        "\n",
        "    top5 = results_df.head(5)\n",
        "    if len(top5) >= 2:\n",
        "        ensemble_candidates = ', '.join(f\"{row.model}@{row.test_auc:.3f}\" for row in top5.itertuples())\n",
        "        print(f\"\\n  Ensemble Option:\")\n",
        "        print(f\"    Soft-vote top 5 models: {ensemble_candidates}\")\n",
        "        print(f\"    Expect +0.001 ~ +0.005 AUC if calibration is consistent\")\n",
        "\n",
        "    manifest_path = SUMMARY_DIR / 'top_model_manifest.csv'\n",
        "    if manifest_path.exists():\n",
        "        family_roots = ', '.join(str(ARTIFACT_ROOT / fam) for fam in MODEL_FAMILIES.keys())\n",
        "        print(f\"\\n  Model Artifacts Manifest: {manifest_path}\")\n",
        "        print(f\"    Per-family binaries stored under: {family_roots}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"PRODUCTION ULTIMATE PIPELINE COMPLETED\")\n",
        "print(f\"Total time: see logs above | Results saved under: {SUMMARY_DIR}\")\n",
        "print(f\"Artifact root (models/data/figures): {ARTIFACT_ROOT.resolve()}\")\n",
        "print(\"=\"*80)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "py310-ai",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
