{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AttriPredict - LR Production Playbook\n",
    "\n",
    "Logistic-regression focused production pipeline with two export-ready variants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Roadmap\n",
    "- Data ingestion and shared preprocessing utilities\n",
    "- Scenario library for three LR production variants\n",
    "- Cross-validated evaluation with seed ensembles and soft voting\n",
    "- Final model export as binary bundles + rich reporting visuals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45db8916",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import json\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    "    precision_recall_curve,\n",
    "    average_precision_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    ")\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.model_selection import StratifiedKFold, RepeatedStratifiedKFold\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, PolynomialFeatures\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "try:\n",
    "    from imblearn.over_sampling import SMOTE, ADASYN, BorderlineSMOTE\n",
    "    from imblearn.combine import SMOTETomek, SMOTEENN\n",
    "    HAS_IMB = True\n",
    "except Exception:\n",
    "    HAS_IMB = False\n",
    "    SMOTE = ADASYN = BorderlineSMOTE = SMOTETomek = SMOTEENN = None\n",
    "    print(\"[WARN] imblearn not available; SMOTE-based recipes will be degraded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f69e4750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artifacts root: D:\\Files\\Develop Projects\\ITHM\\Projects\\AttriPredict\\src\\02_lr_production_playbook\\artifacts\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Paths and configuration\n",
    "DATA_DIR = Path(\"../../data\")\n",
    "OUTPUT_DIR = Path(\"artifacts\")\n",
    "SUMMARY_DIR = OUTPUT_DIR / \"summaries\"\n",
    "\n",
    "for path in [OUTPUT_DIR, SUMMARY_DIR]:\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SCENARIO_DIR_CACHE: Dict[str, Dict[str, Path]] = {}\n",
    "\n",
    "def get_scenario_dirs(name: str) -> Dict[str, Path]:\n",
    "    if name not in SCENARIO_DIR_CACHE:\n",
    "        base = OUTPUT_DIR / name\n",
    "        layout = {\n",
    "            \"base\": base,\n",
    "            \"models\": base / \"models\",\n",
    "            \"figures\": base / \"figures\",\n",
    "            \"data\": base / \"data\",\n",
    "        }\n",
    "        for path in layout.values():\n",
    "            path.mkdir(parents=True, exist_ok=True)\n",
    "        SCENARIO_DIR_CACHE[name] = layout\n",
    "    return SCENARIO_DIR_CACHE[name]\n",
    "\n",
    "TARGET_COL = \"Attrition\"\n",
    "NOMINAL_FEATURES = [\n",
    "    \"BusinessTravel\",\n",
    "    \"Department\",\n",
    "    \"EducationField\",\n",
    "    \"Gender\",\n",
    "    \"JobRole\",\n",
    "    \"MaritalStatus\",\n",
    "    \"OverTime\",\n",
    "]\n",
    "RANDOM_SEEDS = [42, 1337, 2024]\n",
    "BASELINE_AUC = 0.8797\n",
    "\n",
    "print(f\"Artifacts root: {OUTPUT_DIR.resolve()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94b1662d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (1100, 27), attrition rate: 0.162\n",
      "Test  shape: (350, 27), attrition rate: 0.151\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "train_df = pd.read_csv(DATA_DIR / \"train.csv\")\n",
    "test_df = pd.read_csv(DATA_DIR / \"test.csv\")\n",
    "\n",
    "drop_cols = [\"Over18\", \"StandardHours\", \"EmployeeNumber\"]\n",
    "train_df = train_df.drop(columns=drop_cols)\n",
    "test_df = test_df.drop(columns=drop_cols)\n",
    "\n",
    "X_train_full = train_df.drop(columns=[TARGET_COL])\n",
    "y_train_full = train_df[TARGET_COL].astype(int)\n",
    "X_test_full = test_df.drop(columns=[TARGET_COL])\n",
    "y_test_full = test_df[TARGET_COL].astype(int)\n",
    "\n",
    "print(f\"Train shape: {X_train_full.shape}, attrition rate: {y_train_full.mean():.3f}\")\n",
    "print(f\"Test  shape: {X_test_full.shape}, attrition rate: {y_test_full.mean():.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be81bdb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_sampler(config: Optional[Dict[str, Any]]):\n",
    "    if not config or not HAS_IMB:\n",
    "        return None\n",
    "    name = config.get(\"name\", \"smote\").lower()\n",
    "    params = dict(config.get(\"params\", {}))\n",
    "    params.setdefault(\"random_state\", config.get(\"random_state\", 42))\n",
    "    if name == \"smote\" and SMOTE is not None:\n",
    "        return SMOTE(**params)\n",
    "    if name == \"adasyn\" and ADASYN is not None:\n",
    "        return ADASYN(**params)\n",
    "    if name == \"borderlinesmote\" and BorderlineSMOTE is not None:\n",
    "        return BorderlineSMOTE(**params)\n",
    "    if name == \"smoteenn\" and SMOTEENN is not None:\n",
    "        return SMOTEENN(**params)\n",
    "    if name == \"smotetomek\" and SMOTETomek is not None:\n",
    "        return SMOTETomek(**params)\n",
    "    raise ValueError(f\"Sampler '{name}' unavailable in current environment.\")\n",
    "\n",
    "\n",
    "def apply_sampler(df: pd.DataFrame, y: np.ndarray, config: Optional[Dict[str, Any]]):\n",
    "    if not config:\n",
    "        return df, y\n",
    "    sampler = build_sampler(config)\n",
    "    X_res, y_res = sampler.fit_resample(df.values, y) # type: ignore\n",
    "    df_res = pd.DataFrame(X_res, columns=df.columns)\n",
    "    return df_res, y_res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89ebacac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeaturePipeline:\n",
    "    def __init__(self, config: Optional[Dict[str, Any]] = None, nominal_features: Optional[List[str]] = None):\n",
    "        self.config = config or {}\n",
    "        self.nominal_features = nominal_features or NOMINAL_FEATURES\n",
    "        self.skew_threshold = self.config.get(\"skew_threshold\", 0.5)\n",
    "\n",
    "        self.encoder: Optional[OneHotEncoder] = None\n",
    "        self.numeric_columns: List[str] = []\n",
    "        self.basic_columns: List[str] = []\n",
    "        self.scaled_columns: List[str] = []\n",
    "        self.skewed_features: List[str] = []\n",
    "        self.log_shifts: Dict[str, float] = {}\n",
    "        self.scaler: Optional[StandardScaler] = None\n",
    "\n",
    "        self.pre_advanced_columns: List[str] = []\n",
    "        self.advanced_added: List[str] = []\n",
    "\n",
    "        self.poly: Optional[PolynomialFeatures] = None\n",
    "        self.poly_base_cols: List[str] = []\n",
    "        self.poly_new_cols: List[str] = []\n",
    "\n",
    "        self.selector = None\n",
    "        self.selected_columns: Optional[List[str]] = None\n",
    "\n",
    "        self.feature_columns: Optional[List[str]] = None\n",
    "\n",
    "    def _build_encoder(self) -> OneHotEncoder:\n",
    "        try:\n",
    "            encoder = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "        except TypeError:\n",
    "            encoder = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
    "        return encoder\n",
    "\n",
    "    def _encoder_feature_names(self) -> List[str]:\n",
    "        if hasattr(self.encoder, \"get_feature_names_out\"):\n",
    "            return list(self.encoder.get_feature_names_out(self.nominal_features)) # type: ignore\n",
    "        return list(self.encoder.get_feature_names(self.nominal_features)) # type: ignore\n",
    "\n",
    "    def _fit_basic(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        X_num = X.drop(columns=self.nominal_features)\n",
    "        self.numeric_columns = X_num.columns.tolist()\n",
    "\n",
    "        self.encoder = self._build_encoder()\n",
    "        encoded = self.encoder.fit_transform(X[self.nominal_features])\n",
    "        encoded_df = pd.DataFrame(encoded, columns=self._encoder_feature_names(), index=X.index)\n",
    "\n",
    "        X_proc = pd.concat([X_num, encoded_df], axis=1)\n",
    "        self.basic_columns = X_proc.columns.tolist()\n",
    "\n",
    "        self.skewed_features = []\n",
    "        self.log_shifts = {}\n",
    "        for col in self.basic_columns:\n",
    "            series = X_proc[col].astype(float)\n",
    "            skew_value = float(series.skew()) # type: ignore\n",
    "            if np.isnan(skew_value):\n",
    "                continue\n",
    "            if abs(skew_value) > self.skew_threshold:\n",
    "                shift = 0.0\n",
    "                min_val = series.min()\n",
    "                if min_val <= -1.0:\n",
    "                    shift = abs(min_val) + 1.0\n",
    "                self.log_shifts[col] = shift\n",
    "                X_proc[col] = np.log1p(series + shift)\n",
    "                self.skewed_features.append(col)\n",
    "\n",
    "        self.scaler = StandardScaler()\n",
    "        scaled = self.scaler.fit_transform(X_proc)\n",
    "        X_scaled = pd.DataFrame(scaled, columns=X_proc.columns, index=X_proc.index)\n",
    "        self.scaled_columns = X_scaled.columns.tolist()\n",
    "        return X_scaled\n",
    "\n",
    "    def _transform_basic(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        X_num = X[self.numeric_columns].copy()\n",
    "        encoded = self.encoder.transform(X[self.nominal_features]) # type: ignore\n",
    "        encoded_df = pd.DataFrame(encoded, columns=self._encoder_feature_names(), index=X.index) # type: ignore\n",
    "\n",
    "        X_proc = pd.concat([X_num, encoded_df], axis=1)\n",
    "        X_proc = X_proc.reindex(columns=self.basic_columns, fill_value=0.0)\n",
    "\n",
    "        for col in self.skewed_features:\n",
    "            shift = self.log_shifts.get(col, 0.0)\n",
    "            X_proc[col] = np.log1p(X_proc[col].astype(float) + shift)\n",
    "\n",
    "        scaled = self.scaler.transform(X_proc) # type: ignore\n",
    "        X_scaled = pd.DataFrame(scaled, columns=X_proc.columns, index=X_proc.index)\n",
    "        return X_scaled\n",
    "\n",
    "    def _generate_advanced(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, List[str]]:\n",
    "        cfg = self.config.get(\"advanced_config\", {})\n",
    "        df_aug = df.copy()\n",
    "        added: List[str] = []\n",
    "\n",
    "        interactions = cfg.get(\n",
    "            \"interaction_pairs\",\n",
    "            [\n",
    "                (\"Age\", \"MonthlyIncome\"),\n",
    "                (\"DistanceFromHome\", \"OverTime_Yes\"),\n",
    "                (\"YearsAtCompany\", \"JobLevel\"),\n",
    "                (\"TotalWorkingYears\", \"Age\"),\n",
    "                (\"WorkLifeBalance\", \"OverTime_Yes\"),\n",
    "                (\"JobSatisfaction\", \"EnvironmentSatisfaction\"),\n",
    "                (\"MonthlyIncome\", \"JobLevel\"),\n",
    "                (\"TotalWorkingYears\", \"NumCompaniesWorked\"),\n",
    "            ],\n",
    "        )\n",
    "        for col1, col2 in interactions:\n",
    "            if col1 in df_aug.columns and col2 in df_aug.columns:\n",
    "                name = f\"{col1}_x_{col2}\"\n",
    "                df_aug[name] = df_aug[col1] * df_aug[col2]\n",
    "                added.append(name)\n",
    "\n",
    "        ratio_specs = cfg.get(\n",
    "            \"ratio_features\",\n",
    "            [\n",
    "                (\"MonthlyIncome\", \"Age\", \"Income_per_Age\"),\n",
    "                (\"YearsAtCompany\", \"TotalWorkingYears\", \"Company_vs_Total_Years\"),\n",
    "                (\"YearsSinceLastPromotion\", \"YearsAtCompany\", \"Promotion_Frequency\"),\n",
    "                (\"YearsWithCurrManager\", \"YearsInCurrentRole\", \"Manager_Stability\"),\n",
    "                (\"TotalWorkingYears\", \"NumCompaniesWorked\", \"Years_per_Company\"),\n",
    "            ],\n",
    "        )\n",
    "        for num_col, denom_col, name in ratio_specs:\n",
    "            if num_col in df_aug.columns and denom_col in df_aug.columns:\n",
    "                df_aug[name] = df_aug[num_col] / (df_aug[denom_col] + 1e-5)\n",
    "                added.append(name)\n",
    "\n",
    "        delta_specs = cfg.get(\n",
    "            \"delta_features\",\n",
    "            [\n",
    "                (\"YearsAtCompany\", \"YearsInCurrentRole\", \"Tenure_vs_Role\"),\n",
    "                (\"YearsAtCompany\", \"YearsWithCurrManager\", \"Tenure_vs_Manager\"),\n",
    "                (\"PerformanceRating\", \"EnvironmentSatisfaction\", \"Performance_minus_Environment\"),\n",
    "            ],\n",
    "        )\n",
    "        for top_col, bottom_col, name in delta_specs:\n",
    "            if top_col in df_aug.columns and bottom_col in df_aug.columns:\n",
    "                df_aug[name] = df_aug[top_col] - df_aug[bottom_col]\n",
    "                added.append(name)\n",
    "\n",
    "        satisfaction_cols = cfg.get(\n",
    "            \"satisfaction_cols\",\n",
    "            [\"JobSatisfaction\", \"EnvironmentSatisfaction\", \"RelationshipSatisfaction\", \"WorkLifeBalance\"],\n",
    "        )\n",
    "        if all(col in df_aug.columns for col in satisfaction_cols):\n",
    "            df_aug[\"Satisfaction_Mean\"] = df_aug[satisfaction_cols].mean(axis=1)\n",
    "            df_aug[\"Satisfaction_Std\"] = df_aug[satisfaction_cols].std(axis=1)\n",
    "            df_aug[\"Satisfaction_Max\"] = df_aug[satisfaction_cols].max(axis=1)\n",
    "            added.extend([\"Satisfaction_Mean\", \"Satisfaction_Std\", \"Satisfaction_Max\"])\n",
    "\n",
    "        return df_aug, added\n",
    "\n",
    "    def _fit_advanced(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        self.pre_advanced_columns = df.columns.tolist()\n",
    "        df_aug, added = self._generate_advanced(df)\n",
    "        self.advanced_added = added\n",
    "        return df_aug\n",
    "\n",
    "    def _transform_advanced(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        df_aug, _ = self._generate_advanced(df)\n",
    "        for col in self.advanced_added:\n",
    "            if col not in df_aug.columns:\n",
    "                df_aug[col] = 0.0\n",
    "        ordered = self.pre_advanced_columns + self.advanced_added\n",
    "        df_aug = df_aug.reindex(columns=ordered, fill_value=0.0)\n",
    "        return df_aug\n",
    "\n",
    "    def _fit_poly(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        cfg = self.config.get(\"poly\") or {}\n",
    "        degree = cfg.get(\"degree\", 2)\n",
    "        interaction_only = cfg.get(\"interaction_only\", False)\n",
    "        include_bias = cfg.get(\"include_bias\", False)\n",
    "        max_base = cfg.get(\"max_base_features\", 12)\n",
    "        base_cols = cfg.get(\"columns\")\n",
    "\n",
    "        if base_cols is None:\n",
    "            variances = df.var().sort_values(ascending=False)\n",
    "            top_n = min(max_base, len(variances))\n",
    "            base_cols = list(variances.head(top_n).index)\n",
    "        base_cols = [col for col in base_cols if col in df.columns]\n",
    "        if not base_cols:\n",
    "            self.poly = None\n",
    "            self.poly_base_cols = []\n",
    "            self.poly_new_cols = []\n",
    "            return df\n",
    "\n",
    "        poly = PolynomialFeatures(\n",
    "            degree=degree,\n",
    "            interaction_only=interaction_only,\n",
    "            include_bias=include_bias,\n",
    "        )\n",
    "        arr = poly.fit_transform(df[base_cols])\n",
    "        names = poly.get_feature_names_out(base_cols)\n",
    "        poly_df = pd.DataFrame(arr, columns=names, index=df.index)\n",
    "\n",
    "        new_cols = [col for col in names if col not in df.columns]\n",
    "        df_aug = pd.concat([df, poly_df[new_cols]], axis=1)\n",
    "\n",
    "        self.poly = poly\n",
    "        self.poly_base_cols = base_cols\n",
    "        self.poly_new_cols = new_cols\n",
    "        return df_aug\n",
    "\n",
    "    def _transform_poly(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        if self.poly is None or not self.poly_base_cols:\n",
    "            return df\n",
    "        poly_arr = self.poly.transform(df[self.poly_base_cols])\n",
    "        names = self.poly.get_feature_names_out(self.poly_base_cols)\n",
    "        poly_df = pd.DataFrame(poly_arr, columns=names, index=df.index) # type: ignore\n",
    "        if self.poly_new_cols:\n",
    "            df_aug = pd.concat([df, poly_df[self.poly_new_cols]], axis=1)\n",
    "        else:\n",
    "            df_aug = df\n",
    "        return df_aug\n",
    "\n",
    "    def _fit_selector(self, df: pd.DataFrame, y: np.ndarray) -> pd.DataFrame:\n",
    "        selector_cfg = self.config.get(\"feature_selector\") or {}\n",
    "        strategy = selector_cfg.get(\"strategy\", \"selectk\").lower()\n",
    "        if strategy == \"selectk\":\n",
    "            k = selector_cfg.get(\"k\", min(120, df.shape[1]))\n",
    "            k = min(k, df.shape[1])\n",
    "            selector = SelectKBest(score_func=f_classif, k=k)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported selector strategy '{strategy}' for FeaturePipeline.\")\n",
    "        arr = selector.fit_transform(df.values, y)\n",
    "        selected_cols = df.columns[selector.get_support()]\n",
    "        df_sel = pd.DataFrame(arr, columns=selected_cols, index=df.index)\n",
    "        self.selector = selector\n",
    "        self.selected_columns = selected_cols.tolist()\n",
    "        return df_sel\n",
    "\n",
    "    def _transform_selector(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        if self.selector is None:\n",
    "            return df\n",
    "        arr = self.selector.transform(df.values)\n",
    "        df_sel = pd.DataFrame(arr, columns=self.selected_columns, index=df.index) # type: ignore\n",
    "        return df_sel\n",
    "\n",
    "    def fit_transform_train(self, X: pd.DataFrame, y: np.ndarray) -> Tuple[pd.DataFrame, np.ndarray]:\n",
    "        samplers = self.config.get(\"samplers\", {}) or {}\n",
    "        y_array = np.asarray(y).ravel()\n",
    "\n",
    "        X_basic = self._fit_basic(X)\n",
    "        self.pre_advanced_columns = X_basic.columns.tolist()\n",
    "        y_current = y_array\n",
    "\n",
    "        if samplers.get(\"basic\"):\n",
    "            X_basic, y_current = apply_sampler(X_basic, y_current, samplers.get(\"basic\"))\n",
    "\n",
    "        X_proc = X_basic\n",
    "        if self.config.get(\"use_advanced\", False):\n",
    "            X_proc = self._fit_advanced(X_proc)\n",
    "        else:\n",
    "            self.advanced_added = []\n",
    "            self.pre_advanced_columns = X_proc.columns.tolist()\n",
    "\n",
    "        if self.config.get(\"poly\"):\n",
    "            X_proc = self._fit_poly(X_proc)\n",
    "        else:\n",
    "            self.poly = None\n",
    "            self.poly_base_cols = []\n",
    "            self.poly_new_cols = []\n",
    "\n",
    "        if self.config.get(\"feature_selector\"):\n",
    "            X_proc = self._fit_selector(X_proc, y_current) # type: ignore\n",
    "        else:\n",
    "            self.selector = None\n",
    "            self.selected_columns = None\n",
    "\n",
    "        if samplers.get(\"advanced\"):\n",
    "            X_proc, y_current = apply_sampler(X_proc, y_current, samplers.get(\"advanced\")) # type: ignore\n",
    "\n",
    "        self.feature_columns = X_proc.columns.tolist()\n",
    "        return X_proc, y_current # type: ignore\n",
    "\n",
    "    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        X_basic = self._transform_basic(X)\n",
    "        if not self.pre_advanced_columns:\n",
    "            self.pre_advanced_columns = X_basic.columns.tolist()\n",
    "\n",
    "        X_proc = X_basic\n",
    "        if self.config.get(\"use_advanced\", False):\n",
    "            X_proc = self._transform_advanced(X_basic)\n",
    "        else:\n",
    "            X_proc = X_basic.reindex(columns=self.pre_advanced_columns, fill_value=0.0)\n",
    "\n",
    "        if self.config.get(\"poly\"):\n",
    "            X_proc = self._transform_poly(X_proc)\n",
    "\n",
    "        if self.selector is not None:\n",
    "            X_proc = self._transform_selector(X_proc)\n",
    "\n",
    "        if self.feature_columns is not None:\n",
    "            X_proc = X_proc.reindex(columns=self.feature_columns, fill_value=0.0)\n",
    "\n",
    "        return X_proc\n",
    "\n",
    "    def get_feature_names(self) -> List[str]:\n",
    "        return list(self.feature_columns or [])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ad73976",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_cv(strategy: Dict[str, Any], seed: int):\n",
    "    mode = strategy.get(\"mode\", \"kfold\")\n",
    "    n_splits = strategy.get(\"n_splits\", 5)\n",
    "    if mode == \"kfold\":\n",
    "        return StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    if mode == \"repeated\":\n",
    "        n_repeats = strategy.get(\"n_repeats\", 3)\n",
    "        return RepeatedStratifiedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=seed)\n",
    "    raise ValueError(f\"Unknown CV mode '{mode}'\")\n",
    "\n",
    "\n",
    "def find_threshold(y_true: np.ndarray, probs: np.ndarray, strategy: str, params: Optional[Dict[str, Any]] = None) -> float:\n",
    "    params = params or {}\n",
    "    if strategy == \"default\":\n",
    "        return 0.5\n",
    "\n",
    "    thresholds = np.linspace(0.05, 0.95, 181)\n",
    "    best_threshold = 0.5\n",
    "    best_score = -np.inf\n",
    "\n",
    "    if strategy == \"max_f1\":\n",
    "        for t in thresholds:\n",
    "            preds = (probs >= t).astype(int)\n",
    "            score = f1_score(y_true, preds, zero_division=0)\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_threshold = float(t)\n",
    "        return best_threshold\n",
    "\n",
    "    if strategy == \"balanced_precision_recall\":\n",
    "        weight = params.get(\"recall_weight\", 0.5)\n",
    "        for t in thresholds:\n",
    "            preds = (probs >= t).astype(int)\n",
    "            prec = precision_score(y_true, preds, zero_division=0)\n",
    "            rec = recall_score(y_true, preds, zero_division=0)\n",
    "            score = weight * rec + (1.0 - weight) * prec\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_threshold = float(t)\n",
    "        return best_threshold\n",
    "\n",
    "    if strategy == \"recall_floor\":\n",
    "        min_recall = params.get(\"min_recall\", 0.75)\n",
    "        best_precision = -np.inf\n",
    "        for t in thresholds:\n",
    "            preds = (probs >= t).astype(int)\n",
    "            rec = recall_score(y_true, preds, zero_division=0)\n",
    "            prec = precision_score(y_true, preds, zero_division=0)\n",
    "            if rec >= min_recall and prec > best_precision:\n",
    "                best_precision = prec\n",
    "                best_threshold = float(t)\n",
    "        return best_threshold\n",
    "\n",
    "    return best_threshold\n",
    "\n",
    "\n",
    "def compute_metrics(y_true: np.ndarray, probs: np.ndarray, threshold: float) -> Dict[str, float]:\n",
    "    preds = (probs >= threshold).astype(int)\n",
    "    return {\n",
    "        \"auc\": float(roc_auc_score(y_true, probs)),\n",
    "        \"pr_auc\": float(average_precision_score(y_true, probs)),\n",
    "        \"precision\": float(precision_score(y_true, preds, zero_division=0)),\n",
    "        \"recall\": float(recall_score(y_true, preds, zero_division=0)),\n",
    "        \"f1\": float(f1_score(y_true, preds, zero_division=0)),\n",
    "    }\n",
    "\n",
    "\n",
    "def plot_roc_curve(ax, y_true: np.ndarray, probs: np.ndarray, label: str):\n",
    "    fpr, tpr, _ = roc_curve(y_true, probs)\n",
    "    auc_value = roc_auc_score(y_true, probs)\n",
    "    ax.plot(fpr, tpr, label=f\"{label} (AUC={auc_value:.3f})\", linewidth=2)\n",
    "    ax.plot([0, 1], [0, 1], linestyle=\"--\", color=\"grey\", alpha=0.6)\n",
    "    ax.set_xlabel(\"False Positive Rate\")\n",
    "    ax.set_ylabel(\"True Positive Rate\")\n",
    "    ax.set_title(\"ROC Curve\")\n",
    "    ax.legend(loc=\"lower right\")\n",
    "    ax.grid(alpha=0.2)\n",
    "\n",
    "\n",
    "def plot_pr_curve(ax, y_true: np.ndarray, probs: np.ndarray, label: str):\n",
    "    precision, recall, _ = precision_recall_curve(y_true, probs)\n",
    "    ap = average_precision_score(y_true, probs)\n",
    "    ax.plot(recall, precision, label=f\"{label} (AP={ap:.3f})\", linewidth=2)\n",
    "    ax.set_xlabel(\"Recall\")\n",
    "    ax.set_ylabel(\"Precision\")\n",
    "    ax.set_title(\"Precision-Recall Curve\")\n",
    "    ax.legend(loc=\"lower left\")\n",
    "    ax.grid(alpha=0.2)\n",
    "\n",
    "\n",
    "def plot_confusion(ax, cm: np.ndarray, labels: List[str], title: str):\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False, ax=ax)\n",
    "    ax.set_xlabel(\"Predicted\")\n",
    "    ax.set_ylabel(\"Actual\")\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax.set_yticklabels(labels)\n",
    "    ax.set_title(title)\n",
    "\n",
    "\n",
    "def plot_cv_distribution(ax, seed_summaries: List[Dict[str, Any]], title: str):\n",
    "    rows = []\n",
    "    for seed_info in seed_summaries:\n",
    "        for fold_idx, score in enumerate(seed_info[\"cv_scores\"]):\n",
    "            rows.append({\"seed\": seed_info[\"seed\"], \"fold\": fold_idx, \"auc\": score})\n",
    "    cv_df = pd.DataFrame(rows)\n",
    "    sns.boxplot(data=cv_df, x=\"seed\", y=\"auc\", ax=ax, palette=\"Set2\")\n",
    "    ax.axhline(BASELINE_AUC, linestyle=\"--\", color=\"grey\", alpha=0.6, label=\"Baseline\")\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"Seed\")\n",
    "    ax.set_ylabel(\"Fold AUC\")\n",
    "    ax.legend()\n",
    "    ax.grid(alpha=0.2)\n",
    "\n",
    "\n",
    "def plot_calibration(ax, y_true: np.ndarray, probs: np.ndarray, label: str, n_bins: int = 10):\n",
    "    prob_true, prob_pred = calibration_curve(y_true, probs, n_bins=n_bins, strategy=\"quantile\")\n",
    "    ax.plot(prob_pred, prob_true, marker=\"o\", linewidth=2, label=label)\n",
    "    ax.plot([0, 1], [0, 1], linestyle=\"--\", color=\"grey\", alpha=0.7, label=\"Perfect calibration\")\n",
    "    ax.set_title(\"Calibration Curve\")\n",
    "    ax.set_xlabel(\"Predicted probability\")\n",
    "    ax.set_ylabel(\"Observed probability\")\n",
    "    ax.legend(loc=\"upper left\")\n",
    "    ax.grid(alpha=0.2)\n",
    "\n",
    "\n",
    "def plot_probability_hist(ax, y_true: np.ndarray, probs: np.ndarray):\n",
    "    df = pd.DataFrame({\"prob\": probs, \"actual\": y_true}).copy()\n",
    "    df[\"actual_label\"] = df[\"actual\"].map({0: \"Stay\", 1: \"Leave\"})\n",
    "    sns.histplot(data=df, x=\"prob\", hue=\"actual_label\", bins=20, stat=\"density\", common_norm=False, alpha=0.6, ax=ax)\n",
    "    ax.set_title(\"Prediction Distribution by Class\")\n",
    "    ax.set_xlabel(\"Predicted probability\")\n",
    "    ax.set_ylabel(\"Density\")\n",
    "    ax.legend(title=\"Actual\")\n",
    "    ax.grid(alpha=0.2)\n",
    "\n",
    "\n",
    "def prepare_cumulative_gain(y_true: np.ndarray, probs: np.ndarray) -> Tuple[pd.DataFrame, float]:\n",
    "    df = pd.DataFrame({\"actual\": y_true, \"prob\": probs}).sort_values(\"prob\", ascending=False).reset_index(drop=True)\n",
    "    df[\"cum_pos\"] = df[\"actual\"].cumsum()\n",
    "    total_pos = df[\"actual\"].sum()\n",
    "    total = len(df)\n",
    "    if total == 0 or total_pos == 0:\n",
    "        frac = np.linspace(0, 1, 11)\n",
    "        res = pd.DataFrame({\"fraction\": frac, \"gain\": frac, \"lift\": np.ones_like(frac)})\n",
    "        return res, 0.0\n",
    "    df[\"fraction\"] = (df.index + 1) / total\n",
    "    df[\"gain\"] = df[\"cum_pos\"] / total_pos\n",
    "    df[\"lift\"] = df[\"gain\"] / np.where(df[\"fraction\"] == 0, np.nan, df[\"fraction\"])\n",
    "    df = df.fillna(1.0)\n",
    "    base_rate = total_pos / total\n",
    "    return df, base_rate\n",
    "\n",
    "\n",
    "def plot_gain_curve(ax, gain_df: pd.DataFrame):\n",
    "    ax.plot(gain_df[\"fraction\"], gain_df[\"gain\"], linewidth=2)\n",
    "    ax.plot([0, 1], [0, 1], linestyle=\"--\", color=\"grey\", alpha=0.6)\n",
    "    ax.set_title(\"Cumulative Gain\")\n",
    "    ax.set_xlabel(\"Fraction of samples\")\n",
    "    ax.set_ylabel(\"Fraction of positives captured\")\n",
    "    ax.grid(alpha=0.2)\n",
    "\n",
    "\n",
    "def plot_lift_curve(ax, gain_df: pd.DataFrame, base_rate: float):\n",
    "    ax.plot(gain_df[\"fraction\"], gain_df[\"lift\"], linewidth=2)\n",
    "    ax.axhline(1.0, linestyle=\"--\", color=\"grey\", alpha=0.6)\n",
    "    if base_rate:\n",
    "        ax.set_title(f\"Lift Curve (base rate={base_rate:.2%})\")\n",
    "    else:\n",
    "        ax.set_title(\"Lift Curve\")\n",
    "    ax.set_xlabel(\"Fraction of samples\")\n",
    "    ax.set_ylabel(\"Lift\")\n",
    "    ax.grid(alpha=0.2)\n",
    "\n",
    "\n",
    "def build_threshold_metrics(y_true: np.ndarray, probs: np.ndarray, thresholds: Optional[np.ndarray] = None) -> pd.DataFrame:\n",
    "    thresholds = thresholds if thresholds is not None else np.linspace(0.1, 0.9, 17)\n",
    "    rows = []\n",
    "    for t in thresholds:\n",
    "        preds = (probs >= t).astype(int)\n",
    "        rows.append(\n",
    "            {\n",
    "                \"threshold\": round(float(t), 3),\n",
    "                \"precision\": float(precision_score(y_true, preds, zero_division=0)),\n",
    "                \"recall\": float(recall_score(y_true, preds, zero_division=0)),\n",
    "                \"f1\": float(f1_score(y_true, preds, zero_division=0)),\n",
    "            }\n",
    "        )\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "def plot_threshold_sweep(ax, metrics_df: pd.DataFrame):\n",
    "    ax.plot(metrics_df[\"threshold\"], metrics_df[\"precision\"], label=\"Precision\", linewidth=2)\n",
    "    ax.plot(metrics_df[\"threshold\"], metrics_df[\"recall\"], label=\"Recall\", linewidth=2)\n",
    "    ax.plot(metrics_df[\"threshold\"], metrics_df[\"f1\"], label=\"F1\", linewidth=2)\n",
    "    ax.set_title(\"Threshold Sweep\")\n",
    "    ax.set_xlabel(\"Threshold\")\n",
    "    ax.set_ylabel(\"Score\")\n",
    "    ax.set_ylim(0, 1.05)\n",
    "    ax.legend(loc=\"best\")\n",
    "    ax.grid(alpha=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22a88471",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Scenario catalog focusing on leaderboard surge vs validation stability\n",
    "LR_SCENARIOS: Dict[str, Dict[str, Any]] = {\n",
    "    \"lr_score_chaser\": {\n",
    "        \"description\": \"Advanced feature stack tuned to chase leaderboard AUC.\",\n",
    "        \"selection_focus\": \"holdout_auc\",\n",
    "        \"feature_pipeline\": {\n",
    "            \"use_advanced\": True,\n",
    "            \"advanced_config\": {},\n",
    "            \"poly\": None,\n",
    "            \"feature_selector\": None,\n",
    "            \"samplers\": {\n",
    "                \"basic\": None,\n",
    "                \"advanced\": {\"name\": \"smote\"},\n",
    "            },\n",
    "        },\n",
    "        \"logistic_params\": {\"C\": 10.0, \"penalty\": \"l2\", \"solver\": \"lbfgs\", \"max_iter\": 4000},\n",
    "        \"cv\": {\"mode\": \"kfold\", \"n_splits\": 5},\n",
    "        \"threshold\": {\"strategy\": \"default\", \"params\": {}},\n",
    "        \"soft_voting\": \"mean\",\n",
    "        \"seeds\": RANDOM_SEEDS,\n",
    "    },\n",
    "    \"lr_validation_guardian\": {\n",
    "        \"description\": \"Lean features with SMOTE and heavier regularisation for validation stability.\",\n",
    "        \"selection_focus\": \"validation_consistency\",\n",
    "        \"feature_pipeline\": {\n",
    "            \"use_advanced\": False,\n",
    "            \"poly\": None,\n",
    "            \"feature_selector\": None,\n",
    "            \"samplers\": {\n",
    "                \"basic\": {\"name\": \"smote\"},\n",
    "                \"advanced\": None,\n",
    "            },\n",
    "        },\n",
    "        \"logistic_params\": {\"C\": 0.1, \"penalty\": \"l2\", \"solver\": \"lbfgs\", \"max_iter\": 4000},\n",
    "        \"cv\": {\"mode\": \"repeated\", \"n_splits\": 5, \"n_repeats\": 3},\n",
    "        \"threshold\": {\"strategy\": \"default\", \"params\": {}},\n",
    "        \"soft_voting\": \"mean\",\n",
    "        \"seeds\": RANDOM_SEEDS,\n",
    "    },\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65d957e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_scenario(name: str, recipe: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    seeds = recipe.get(\"seeds\", RANDOM_SEEDS)\n",
    "    seed_summaries: List[Dict[str, Any]] = []\n",
    "\n",
    "    for seed in seeds:\n",
    "        cv = build_cv(recipe.get(\"cv\", {\"mode\": \"kfold\", \"n_splits\": 5}), seed)\n",
    "        fold_scores: List[float] = []\n",
    "        test_prob = np.zeros(len(X_test_full))\n",
    "        oof_probs = np.zeros(len(X_train_full))\n",
    "\n",
    "        for fold_idx, (train_idx, val_idx) in enumerate(cv.split(X_train_full, y_train_full)):\n",
    "            X_tr = X_train_full.iloc[train_idx]\n",
    "            y_tr = y_train_full.iloc[train_idx].values\n",
    "            X_val = X_train_full.iloc[val_idx]\n",
    "            y_val = y_train_full.iloc[val_idx].values\n",
    "\n",
    "            pipeline = FeaturePipeline(recipe[\"feature_pipeline\"])\n",
    "            X_tr_proc, y_tr_proc = pipeline.fit_transform_train(X_tr, y_tr)\n",
    "\n",
    "            X_val_proc = pipeline.transform(X_val)\n",
    "            X_test_proc = pipeline.transform(X_test_full)\n",
    "\n",
    "            log_params = dict(recipe[\"logistic_params\"])\n",
    "            log_params.setdefault(\"random_state\", seed)\n",
    "\n",
    "            model = LogisticRegression(**log_params)\n",
    "            model.fit(X_tr_proc, y_tr_proc)\n",
    "\n",
    "            val_prob = model.predict_proba(X_val_proc)[:, 1]\n",
    "            oof_probs[val_idx] = val_prob\n",
    "            fold_auc = roc_auc_score(y_val, val_prob)\n",
    "            fold_scores.append(float(fold_auc))\n",
    "\n",
    "            test_prob += model.predict_proba(X_test_proc)[:, 1]\n",
    "\n",
    "        test_prob /= cv.get_n_splits()\n",
    "\n",
    "        seed_summaries.append(\n",
    "            {\n",
    "                \"seed\": seed,\n",
    "                \"cv_scores\": fold_scores,\n",
    "                \"cv_mean\": float(np.mean(fold_scores)),\n",
    "                \"cv_std\": float(np.std(fold_scores)),\n",
    "                \"oof_probs\": oof_probs,\n",
    "                \"test_probs\": test_prob,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    oof_stack = np.vstack([s[\"oof_probs\"] for s in seed_summaries])\n",
    "    ensemble_oof = oof_stack.mean(axis=0)\n",
    "\n",
    "    threshold_cfg = recipe.get(\"threshold\", {\"strategy\": \"default\", \"params\": {}})\n",
    "    threshold = find_threshold(\n",
    "        y_train_full.values, # type: ignore\n",
    "        ensemble_oof,\n",
    "        threshold_cfg.get(\"strategy\", \"default\"),\n",
    "        threshold_cfg.get(\"params\"),\n",
    "    )\n",
    "\n",
    "    test_stack = np.vstack([s[\"test_probs\"] for s in seed_summaries])\n",
    "    if recipe.get(\"soft_voting\", \"mean\") == \"median\":\n",
    "        ensemble_probs = np.median(test_stack, axis=0)\n",
    "    else:\n",
    "        ensemble_probs = np.mean(test_stack, axis=0)\n",
    "\n",
    "    metrics = compute_metrics(y_test_full.values, ensemble_probs, threshold) # type: ignore\n",
    "    preds = (ensemble_probs >= threshold).astype(int)\n",
    "    cm = confusion_matrix(y_test_full.values, preds) # type: ignore\n",
    "\n",
    "    scenario_result = {\n",
    "        \"scenario\": name,\n",
    "        \"description\": recipe[\"description\"],\n",
    "        \"threshold\": float(threshold),\n",
    "        \"test_auc\": metrics[\"auc\"],\n",
    "        \"test_pr_auc\": metrics[\"pr_auc\"],\n",
    "        \"test_precision\": metrics[\"precision\"],\n",
    "        \"test_recall\": metrics[\"recall\"],\n",
    "        \"test_f1\": metrics[\"f1\"],\n",
    "        \"seed_cv_mean\": float(np.mean([s[\"cv_mean\"] for s in seed_summaries])),\n",
    "        \"seed_cv_std\": float(np.mean([s[\"cv_std\"] for s in seed_summaries])),\n",
    "        \"seeds\": seed_summaries,\n",
    "        \"ensemble_probs\": ensemble_probs,\n",
    "        \"ensemble_oof\": ensemble_oof,\n",
    "        \"confusion_matrix\": cm,\n",
    "        \"classification_report\": classification_report(\n",
    "            y_test_full.values, # type: ignore\n",
    "            preds,\n",
    "            zero_division=0,\n",
    "            output_dict=True,\n",
    "        ),\n",
    "        \"recipe\": recipe,\n",
    "    }\n",
    "    return scenario_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9815b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Running scenario: lr_score_chaser -> Advanced feature stack tuned to chase leaderboard AUC.\n",
      "================================================================================\n",
      "Running scenario: lr_validation_guardian -> Lean features with SMOTE and heavier regularisation for validation stability.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scenario</th>\n",
       "      <th>description</th>\n",
       "      <th>selection_focus</th>\n",
       "      <th>threshold</th>\n",
       "      <th>test_auc</th>\n",
       "      <th>test_pr_auc</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>cv_mean</th>\n",
       "      <th>cv_std</th>\n",
       "      <th>cv_mean_minus_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lr_score_chaser</td>\n",
       "      <td>Advanced feature stack tuned to chase leaderbo...</td>\n",
       "      <td>holdout_auc</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.891</td>\n",
       "      <td>0.6971</td>\n",
       "      <td>0.4175</td>\n",
       "      <td>0.8113</td>\n",
       "      <td>0.5513</td>\n",
       "      <td>0.7952</td>\n",
       "      <td>0.0393</td>\n",
       "      <td>0.7559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lr_validation_guardian</td>\n",
       "      <td>Lean features with SMOTE and heavier regularis...</td>\n",
       "      <td>validation_consistency</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.889</td>\n",
       "      <td>0.6857</td>\n",
       "      <td>0.4074</td>\n",
       "      <td>0.8302</td>\n",
       "      <td>0.5466</td>\n",
       "      <td>0.8232</td>\n",
       "      <td>0.0384</td>\n",
       "      <td>0.7848</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 scenario                                        description  \\\n",
       "0         lr_score_chaser  Advanced feature stack tuned to chase leaderbo...   \n",
       "1  lr_validation_guardian  Lean features with SMOTE and heavier regularis...   \n",
       "\n",
       "          selection_focus  threshold  test_auc  test_pr_auc  precision  \\\n",
       "0             holdout_auc        0.5     0.891       0.6971     0.4175   \n",
       "1  validation_consistency        0.5     0.889       0.6857     0.4074   \n",
       "\n",
       "   recall      f1  cv_mean  cv_std  cv_mean_minus_std  \n",
       "0  0.8113  0.5513   0.7952  0.0393             0.7559  \n",
       "1  0.8302  0.5466   0.8232  0.0384             0.7848  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "scenario_outputs: Dict[str, Dict[str, Any]] = {}\n",
    "summary_rows: List[Dict[str, Any]] = []\n",
    "\n",
    "for scenario_name, scenario_cfg in LR_SCENARIOS.items():\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Running scenario: {scenario_name} -> {scenario_cfg['description']}\")\n",
    "    result = evaluate_scenario(scenario_name, scenario_cfg)\n",
    "    scenario_outputs[scenario_name] = result\n",
    "    summary_rows.append(\n",
    "        {\n",
    "            \"scenario\": scenario_name,\n",
    "            \"description\": scenario_cfg[\"description\"],\n",
    "            \"selection_focus\": scenario_cfg.get(\"selection_focus\", \"holdout_auc\"),\n",
    "            \"threshold\": round(result[\"threshold\"], 4),\n",
    "            \"test_auc\": round(result[\"test_auc\"], 4),\n",
    "            \"test_pr_auc\": round(result[\"test_pr_auc\"], 4),\n",
    "            \"precision\": round(result[\"test_precision\"], 4),\n",
    "            \"recall\": round(result[\"test_recall\"], 4),\n",
    "            \"f1\": round(result[\"test_f1\"], 4),\n",
    "            \"cv_mean\": round(result[\"seed_cv_mean\"], 4),\n",
    "            \"cv_std\": round(result[\"seed_cv_std\"], 4),\n",
    "            \"cv_mean_minus_std\": round(result[\"seed_cv_mean\"] - result[\"seed_cv_std\"], 4),\n",
    "        }\n",
    "    )\n",
    "\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "summary_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6433f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary saved to artifacts\\summaries\\lr_scenario_summary.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "summary_path = SUMMARY_DIR / \"lr_scenario_summary.csv\"\n",
    "summary_df.to_csv(summary_path, index=False)\n",
    "print(f\"Summary saved to {summary_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce95d139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[figure] artifacts\\lr_score_chaser\\figures\\dashboard.png\n",
      "[figure] artifacts\\lr_validation_guardian\\figures\\dashboard.png\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for scenario_name, result in scenario_outputs.items():\n",
    "    artifact_dirs = get_scenario_dirs(scenario_name)\n",
    "    probs = result[\"ensemble_probs\"]\n",
    "    threshold = result[\"threshold\"]\n",
    "    cm = result[\"confusion_matrix\"]\n",
    "\n",
    "    threshold_metrics_df = result.get(\"threshold_metrics_df\")\n",
    "    if threshold_metrics_df is None:\n",
    "        threshold_metrics_df = build_threshold_metrics(y_test_full.values, probs) # type: ignore\n",
    "    gain_df = result.get(\"gain_curve_df\")\n",
    "    base_rate = result.get(\"base_rate\")\n",
    "    if gain_df is None:\n",
    "        gain_df, base_rate = prepare_cumulative_gain(y_test_full.values, probs) # type: ignore\n",
    "\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(20, 18))\n",
    "    plot_roc_curve(axes[0, 0], y_test_full.values, probs, scenario_name) # type: ignore\n",
    "    plot_pr_curve(axes[0, 1], y_test_full.values, probs, scenario_name) # type: ignore\n",
    "    plot_confusion(axes[0, 2], cm, labels=[\"Stay\", \"Leave\"], title=\"Confusion Matrix\") # type: ignore\n",
    "\n",
    "    plot_cv_distribution(axes[1, 0], result[\"seeds\"], title=\"CV Fold Distribution\")\n",
    "    plot_calibration(axes[1, 1], y_test_full.values, probs, scenario_name) # type: ignore\n",
    "    plot_probability_hist(axes[1, 2], y_test_full.values, probs) # type: ignore\n",
    "\n",
    "    plot_gain_curve(axes[2, 0], gain_df)\n",
    "    plot_lift_curve(axes[2, 1], gain_df, base_rate if base_rate is not None else float(y_test_full.mean()))\n",
    "    plot_threshold_sweep(axes[2, 2], threshold_metrics_df)\n",
    "\n",
    "    fig.suptitle(\n",
    "        f\"{scenario_name}: {result['description']}\\n\"\n",
    "        f\"Threshold={threshold:.3f}\",\n",
    "        fontsize=16,\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "    fig.tight_layout(rect=[0, 0.03, 1, 0.97]) # type: ignore\n",
    "\n",
    "    dashboard_path = artifact_dirs[\"figures\"] / \"dashboard.png\"\n",
    "    fig.savefig(dashboard_path, dpi=150, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "    print(f\"[figure] {dashboard_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1acb2c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_final_bundle(name: str, recipe: Dict[str, Any], threshold: float, artifact_dirs: Dict[str, Path]) -> Dict[str, Any]:\n",
    "    seeds = recipe.get(\"seeds\", RANDOM_SEEDS)\n",
    "    bundle = {\n",
    "        \"scenario\": name,\n",
    "        \"description\": recipe[\"description\"],\n",
    "        \"trained_at\": datetime.now().isoformat(),\n",
    "        \"threshold\": float(threshold),\n",
    "        \"soft_voting\": recipe.get(\"soft_voting\", \"mean\"),\n",
    "        \"models\": [],\n",
    "        \"artifact_dirs\": {key: str(val) for key, val in artifact_dirs.items()},\n",
    "    }\n",
    "\n",
    "    for seed in seeds:\n",
    "        pipeline = FeaturePipeline(recipe[\"feature_pipeline\"])\n",
    "        X_proc, y_proc = pipeline.fit_transform_train(X_train_full, y_train_full.values) # type: ignore\n",
    "\n",
    "        log_params = dict(recipe[\"logistic_params\"])\n",
    "        log_params.setdefault(\"random_state\", seed)\n",
    "\n",
    "        model = LogisticRegression(**log_params)\n",
    "        model.fit(X_proc, y_proc)\n",
    "\n",
    "        bundle[\"models\"].append(\n",
    "            {\n",
    "                \"seed\": seed,\n",
    "                \"pipeline\": pipeline,\n",
    "                \"model\": model,\n",
    "                \"feature_names\": pipeline.get_feature_names(),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    path = artifact_dirs[\"models\"] / f\"{name}.pkl\"\n",
    "    joblib.dump(bundle, path)\n",
    "    print(f\"[saved] {path}\")\n",
    "    return bundle\n",
    "\n",
    "\n",
    "def score_bundle(bundle: Dict[str, Any], X: pd.DataFrame, y: np.ndarray) -> Tuple[np.ndarray, Dict[str, float], np.ndarray]:\n",
    "    prob_stack = []\n",
    "    for entry in bundle[\"models\"]:\n",
    "        pipeline = entry[\"pipeline\"]\n",
    "        model = entry[\"model\"]\n",
    "        X_proc = pipeline.transform(X)\n",
    "        prob_stack.append(model.predict_proba(X_proc)[:, 1])\n",
    "    prob_stack = np.vstack(prob_stack)\n",
    "\n",
    "    if bundle.get(\"soft_voting\", \"mean\") == \"median\":\n",
    "        probs = np.median(prob_stack, axis=0)\n",
    "    else:\n",
    "        probs = np.mean(prob_stack, axis=0)\n",
    "\n",
    "    metrics = compute_metrics(y, probs, bundle[\"threshold\"])\n",
    "    preds = (probs >= bundle[\"threshold\"]).astype(int)\n",
    "    cm = confusion_matrix(y, preds)\n",
    "    return probs, metrics, cm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "632bed7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] artifacts\\lr_score_chaser\\models\\lr_score_chaser.pkl\n",
      "[figure] artifacts\\lr_score_chaser\\figures\\coefficients.png\n",
      "[report] artifacts\\lr_score_chaser\\data\\classification_report.json\n",
      "[matrix] artifacts\\lr_score_chaser\\data\\confusion_matrix.csv\n",
      "[data] artifacts\\lr_score_chaser\\data\\test_predictions.csv\n",
      "[data] artifacts\\lr_score_chaser\\data\\cv_fold_scores.csv\n",
      "[data] artifacts\\lr_score_chaser\\data\\threshold_sweep.csv\n",
      "[data] artifacts\\lr_score_chaser\\data\\cumulative_gain.csv\n",
      "[saved] artifacts\\lr_validation_guardian\\models\\lr_validation_guardian.pkl\n",
      "[figure] artifacts\\lr_validation_guardian\\figures\\coefficients.png\n",
      "[report] artifacts\\lr_validation_guardian\\data\\classification_report.json\n",
      "[matrix] artifacts\\lr_validation_guardian\\data\\confusion_matrix.csv\n",
      "[data] artifacts\\lr_validation_guardian\\data\\test_predictions.csv\n",
      "[data] artifacts\\lr_validation_guardian\\data\\cv_fold_scores.csv\n",
      "[data] artifacts\\lr_validation_guardian\\data\\threshold_sweep.csv\n",
      "[data] artifacts\\lr_validation_guardian\\data\\cumulative_gain.csv\n",
      "Bundle summary written to artifacts\\summaries\\lr_bundle_summary.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scenario</th>\n",
       "      <th>threshold</th>\n",
       "      <th>test_auc</th>\n",
       "      <th>test_pr_auc</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>artifact_base</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lr_score_chaser</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.8849</td>\n",
       "      <td>0.6740</td>\n",
       "      <td>0.4057</td>\n",
       "      <td>0.8113</td>\n",
       "      <td>0.5409</td>\n",
       "      <td>artifacts\\lr_score_chaser</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lr_validation_guardian</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.8792</td>\n",
       "      <td>0.6668</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.7925</td>\n",
       "      <td>0.5316</td>\n",
       "      <td>artifacts\\lr_validation_guardian</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 scenario  threshold  test_auc  test_pr_auc  precision  \\\n",
       "0         lr_score_chaser        0.5    0.8849       0.6740     0.4057   \n",
       "1  lr_validation_guardian        0.5    0.8792       0.6668     0.4000   \n",
       "\n",
       "   recall      f1                     artifact_base  \n",
       "0  0.8113  0.5409         artifacts\\lr_score_chaser  \n",
       "1  0.7925  0.5316  artifacts\\lr_validation_guardian  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model_bundles: Dict[str, Dict[str, Any]] = {}\n",
    "bundle_rows: List[Dict[str, Any]] = []\n",
    "\n",
    "for scenario_name, result in scenario_outputs.items():\n",
    "    artifact_dirs = get_scenario_dirs(scenario_name)\n",
    "    bundle = train_final_bundle(scenario_name, LR_SCENARIOS[scenario_name], result[\"threshold\"], artifact_dirs)\n",
    "    model_bundles[scenario_name] = bundle\n",
    "\n",
    "    probs, metrics, cm = score_bundle(bundle, X_test_full, y_test_full.values) # type: ignore\n",
    "    preds = (probs >= bundle[\"threshold\"]).astype(int)\n",
    "    bundle_rows.append(\n",
    "        {\n",
    "            \"scenario\": scenario_name,\n",
    "            \"threshold\": round(bundle[\"threshold\"], 4),\n",
    "            \"test_auc\": round(metrics[\"auc\"], 4),\n",
    "            \"test_pr_auc\": round(metrics[\"pr_auc\"], 4),\n",
    "            \"precision\": round(metrics[\"precision\"], 4),\n",
    "            \"recall\": round(metrics[\"recall\"], 4),\n",
    "            \"f1\": round(metrics[\"f1\"], 4),\n",
    "            \"artifact_base\": str(artifact_dirs[\"base\"]),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    coef_records = []\n",
    "    for entry in bundle[\"models\"]:\n",
    "        feature_names = entry[\"feature_names\"]\n",
    "        coefs = entry[\"model\"].coef_.ravel()\n",
    "        coef_records.extend(\n",
    "            {\"seed\": entry[\"seed\"], \"feature\": feat, \"coef\": coef_val}\n",
    "            for feat, coef_val in zip(feature_names, coefs)\n",
    "        )\n",
    "\n",
    "    coef_df = pd.DataFrame(coef_records)\n",
    "    coef_mean = coef_df.groupby(\"feature\")[\"coef\"].mean().sort_values()\n",
    "\n",
    "    top_neg = coef_mean.head(15)\n",
    "    top_pos = coef_mean.tail(15)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    combined = pd.concat([top_neg, top_pos])\n",
    "    colors = [\"#1f77b4\" if val < 0 else \"#d62728\" for val in combined.values]\n",
    "    combined.plot(kind=\"barh\", ax=ax, color=colors)\n",
    "    ax.axvline(0, color=\"black\", linewidth=1)\n",
    "    ax.set_title(f\"{scenario_name}: mean coefficients (top +/-)\")\n",
    "    ax.set_xlabel(\"Coefficient value\")\n",
    "    fig.tight_layout()\n",
    "    coef_path = artifact_dirs[\"figures\"] / \"coefficients.png\"\n",
    "    fig.savefig(coef_path, dpi=150, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "    print(f\"[figure] {coef_path}\")\n",
    "\n",
    "    report_path = artifact_dirs[\"data\"] / \"classification_report.json\"\n",
    "    final_report = classification_report(\n",
    "        y_test_full.values, # type: ignore\n",
    "        preds,\n",
    "        zero_division=0,\n",
    "        output_dict=True,\n",
    "    )\n",
    "    with open(report_path, \"w\", encoding=\"utf-8\") as fp:\n",
    "        json.dump(final_report, fp, indent=2)\n",
    "    print(f\"[report] {report_path}\")\n",
    "\n",
    "    cm_path = artifact_dirs[\"data\"] / \"confusion_matrix.csv\"\n",
    "    cm_df = pd.DataFrame(cm, index=[\"Actual_Stay\", \"Actual_Leave\"], columns=[\"Pred_Stay\", \"Pred_Leave\"])\n",
    "    cm_df.to_csv(cm_path)\n",
    "    print(f\"[matrix] {cm_path}\")\n",
    "\n",
    "    predictions_df = pd.DataFrame(\n",
    "        {\n",
    "            \"row_index\": X_test_full.index,\n",
    "            \"predicted_probability\": probs,\n",
    "            \"prediction\": preds,\n",
    "            \"actual\": y_test_full.values,\n",
    "        }\n",
    "    )\n",
    "    pred_path = artifact_dirs[\"data\"] / \"test_predictions.csv\"\n",
    "    predictions_df.to_csv(pred_path, index=False)\n",
    "    print(f\"[data] {pred_path}\")\n",
    "\n",
    "    cv_rows = []\n",
    "    for seed_info in result[\"seeds\"]:\n",
    "        for fold_idx, score in enumerate(seed_info[\"cv_scores\"]):\n",
    "            cv_rows.append({\"seed\": seed_info[\"seed\"], \"fold\": fold_idx, \"auc\": score})\n",
    "    cv_df = pd.DataFrame(cv_rows)\n",
    "    cv_path = artifact_dirs[\"data\"] / \"cv_fold_scores.csv\"\n",
    "    cv_df.to_csv(cv_path, index=False)\n",
    "    print(f\"[data] {cv_path}\")\n",
    "\n",
    "    threshold_metrics_df = build_threshold_metrics(y_test_full.values, probs) # type: ignore\n",
    "    threshold_path = artifact_dirs[\"data\"] / \"threshold_sweep.csv\"\n",
    "    threshold_metrics_df.to_csv(threshold_path, index=False)\n",
    "    print(f\"[data] {threshold_path}\")\n",
    "\n",
    "    gain_df, base_rate = prepare_cumulative_gain(y_test_full.values, probs) # type: ignore\n",
    "    gain_path = artifact_dirs[\"data\"] / \"cumulative_gain.csv\"\n",
    "    gain_df.to_csv(gain_path, index=False)\n",
    "    print(f\"[data] {gain_path}\")\n",
    "\n",
    "    scenario_outputs[scenario_name][\"test_predictions\"] = predictions_df\n",
    "    scenario_outputs[scenario_name][\"threshold_metrics_df\"] = threshold_metrics_df\n",
    "    scenario_outputs[scenario_name][\"gain_curve_df\"] = gain_df\n",
    "    scenario_outputs[scenario_name][\"base_rate\"] = base_rate\n",
    "    scenario_outputs[scenario_name][\"cv_folds_df\"] = cv_df\n",
    "\n",
    "bundle_summary_df = pd.DataFrame(bundle_rows).sort_values(\"test_auc\", ascending=False)\n",
    "bundle_summary_path = SUMMARY_DIR / \"lr_bundle_summary.csv\"\n",
    "bundle_summary_df.to_csv(bundle_summary_path, index=False)\n",
    "print(f\"Bundle summary written to {bundle_summary_path}\")\n",
    "bundle_summary_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enriched summary written to artifacts\\summaries\\lr_scenario_summary.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "summary_df = summary_df.merge(bundle_summary_df[[\"scenario\", \"artifact_base\"]], on=\"scenario\", how=\"left\")\n",
    "\n",
    "score_pick = summary_df.loc[summary_df[\"selection_focus\"] == \"holdout_auc\"].sort_values(\"test_auc\", ascending=False).head(1)\n",
    "stability_pick = summary_df.loc[summary_df[\"selection_focus\"] == \"validation_consistency\"].sort_values(\"cv_mean_minus_std\", ascending=False).head(1)\n",
    "\n",
    "recommendations = pd.concat(\n",
    "    [\n",
    "        score_pick.assign(recommended_for=\"Leaderboard score\"),\n",
    "        stability_pick.assign(recommended_for=\"Validation stability\"),\n",
    "    ],\n",
    "    ignore_index=True,\n",
    ")\n",
    "recommendations = recommendations[\n",
    "    [\n",
    "        \"recommended_for\",\n",
    "        \"scenario\",\n",
    "        \"test_auc\",\n",
    "        \"test_pr_auc\",\n",
    "        \"precision\",\n",
    "        \"recall\",\n",
    "        \"f1\",\n",
    "        \"cv_mean\",\n",
    "        \"cv_std\",\n",
    "        \"cv_mean_minus_std\",\n",
    "        \"artifact_base\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "summary_df.to_csv(summary_path, index=False)\n",
    "print(f\"Enriched summary written to {summary_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
